{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### James Cooper | CSC 529 | Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\James Cooper\\Desktop\\DePaul\\Advanced Data Mining\\Homework3\n"
     ]
    }
   ],
   "source": [
    "cd \"C:/Users/James Cooper/Desktop/DePaul/Advanced Data Mining/Homework3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>11.91</td>\n",
       "      <td>49.02</td>\n",
       "      <td>106.96</td>\n",
       "      <td>152.14</td>\n",
       "      <td>157.77</td>\n",
       "      <td>...</td>\n",
       "      <td>145.84</td>\n",
       "      <td>154.03</td>\n",
       "      <td>152.60</td>\n",
       "      <td>142.38</td>\n",
       "      <td>125.40</td>\n",
       "      <td>100.57</td>\n",
       "      <td>69.80</td>\n",
       "      <td>38.67</td>\n",
       "      <td>14.58</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.31</td>\n",
       "      <td>19.03</td>\n",
       "      <td>51.45</td>\n",
       "      <td>87.66</td>\n",
       "      <td>106.14</td>\n",
       "      <td>102.35</td>\n",
       "      <td>96.37</td>\n",
       "      <td>106.66</td>\n",
       "      <td>...</td>\n",
       "      <td>125.28</td>\n",
       "      <td>141.59</td>\n",
       "      <td>145.34</td>\n",
       "      <td>135.52</td>\n",
       "      <td>110.81</td>\n",
       "      <td>74.17</td>\n",
       "      <td>36.65</td>\n",
       "      <td>11.53</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.86</td>\n",
       "      <td>12.63</td>\n",
       "      <td>42.03</td>\n",
       "      <td>84.17</td>\n",
       "      <td>104.87</td>\n",
       "      <td>79.06</td>\n",
       "      <td>32.90</td>\n",
       "      <td>...</td>\n",
       "      <td>158.29</td>\n",
       "      <td>150.08</td>\n",
       "      <td>134.27</td>\n",
       "      <td>113.54</td>\n",
       "      <td>85.57</td>\n",
       "      <td>52.14</td>\n",
       "      <td>23.76</td>\n",
       "      <td>7.75</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.93</td>\n",
       "      <td>33.21</td>\n",
       "      <td>75.00</td>\n",
       "      <td>85.97</td>\n",
       "      <td>51.36</td>\n",
       "      <td>14.25</td>\n",
       "      <td>...</td>\n",
       "      <td>157.26</td>\n",
       "      <td>156.53</td>\n",
       "      <td>139.58</td>\n",
       "      <td>104.68</td>\n",
       "      <td>63.12</td>\n",
       "      <td>28.38</td>\n",
       "      <td>8.15</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>8.57</td>\n",
       "      <td>32.06</td>\n",
       "      <td>63.96</td>\n",
       "      <td>74.10</td>\n",
       "      <td>50.12</td>\n",
       "      <td>18.39</td>\n",
       "      <td>2.83</td>\n",
       "      <td>...</td>\n",
       "      <td>93.12</td>\n",
       "      <td>87.66</td>\n",
       "      <td>59.11</td>\n",
       "      <td>26.47</td>\n",
       "      <td>8.72</td>\n",
       "      <td>2.48</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1     2      3      4      5       6       7       8       9    ...   \\\n",
       "0  0.0  0.0  0.00   0.00   0.95  11.91   49.02  106.96  152.14  157.77  ...    \n",
       "1  0.0  0.0  3.31  19.03  51.45  87.66  106.14  102.35   96.37  106.66  ...    \n",
       "2  0.0  0.0  0.02   1.86  12.63  42.03   84.17  104.87   79.06   32.90  ...    \n",
       "3  0.0  0.0  0.00   0.01   5.93  33.21   75.00   85.97   51.36   14.25  ...    \n",
       "4  0.0  0.0  0.94   8.57  32.06  63.96   74.10   50.12   18.39    2.83  ...    \n",
       "\n",
       "      246     247     248     249     250     251    252    253    254   255  \n",
       "0  145.84  154.03  152.60  142.38  125.40  100.57  69.80  38.67  14.58  2.80  \n",
       "1  125.28  141.59  145.34  135.52  110.81   74.17  36.65  11.53   1.81  0.07  \n",
       "2  158.29  150.08  134.27  113.54   85.57   52.14  23.76   7.75   1.76  0.23  \n",
       "3  157.26  156.53  139.58  104.68   63.12   28.38   8.15   1.11   0.02  0.00  \n",
       "4   93.12   87.66   59.11   26.47    8.72    2.48   0.42   0.00   0.00  0.00  \n",
       "\n",
       "[5 rows x 256 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usps_data = pd.read_csv('uspsdata.txt', delimiter='\\t', header=None)\n",
    "usps_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usps_labels = pd.read_csv('uspscl.txt', delimiter='\\t', header=None)\n",
    "usps_labels.head()\n",
    "shape(usps_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "#### (Handwriting recognition using support vector machines) In this problem, you will apply a support vector machine to classify hand-written digits. Download the digit data set from the course documents for week 6. The zip archive contains two text files. The file uspsdata.txt contains a matrix with one digit/data point (= vector of length 256) per row. The 256-vector in each row represents a 16 by 16 image of a handwritten number. The file uspscl.txt contains the corresponding class labels. The data contains two classes, the digits 5 and 6, and the class labels are stored as -1 and +1, respectively. \n",
    "\n",
    "#### a. Train a linear SVM with soft margin. Vary the soft margin parameter and plot the classification error as a function of the margin parameter. Discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\James Cooper\\Anaconda_9-17\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(usps_data, usps_labels, test_size=0.2, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[0.97499999999999998, 0.97499999999999998, 0.97499999999999998, 0.97499999999999998, 0.97499999999999998, 0.97499999999999998, 0.97499999999999998, 0.97499999999999998, 0.97499999999999998, 0.97499999999999998]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "scores_train_svc = []\n",
    "scores_test_svc = []\n",
    "\n",
    "for i in np.arange(0.1, 10, 1):\n",
    "    \n",
    "    clf = svm.SVC(kernel='linear', C=i)\n",
    "    clf.fit(X_train, y_train)\n",
    "    #clf.predict(X_test)\n",
    "    \n",
    "    y_val_train = y_train\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    \n",
    "    y_val = y_test\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    scores_train = accuracy_score(y_val_train, y_pred_train)\n",
    "    scores_train_svc.append(scores_train)\n",
    "    \n",
    "    scores_test = accuracy_score(y_val, y_pred)\n",
    "    scores_test_svc.append(scores_test)\n",
    "\n",
    "print(scores_train_svc)\n",
    "print(scores_test_svc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = np.arange(0.1, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x4702128>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFJCAYAAAC2OXUDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHnxJREFUeJzt3W9UVPe97/EPzKgtzMBgJabnRCEaMQb10olJ9Vas6QnR\npE1aVAJyj4loW9uqjS6aKDQqKQZIk2XFxED+2GWuFoO9tTac3JXcA01CitRjvQFFoi3qZQWbWBHS\nCKTCMPs+cHVS65+JGZjxN75fjxz2zOzf/i7Wes/ezmJHWJZlCQAAXPUiQ70AAADw6RBtAAAMQbQB\nADAE0QYAwBBEGwAAQxBtAAAMYQ/1Ai7n1KkzAb0+Li5KnZ09A7QaXApzDh5mHRzMOTiY88XFxzsv\nuS2sz7Ttdluol3BNYM7Bw6yDgzkHB3O+cmEdbQAAwgnRBgDAEEQbAABDEG0AAAxBtAEAMATRBgDA\nEEQbAABDEG0AAAzxqaLd2NioBQsWXPDz3/72t5o7d64yMzO1c+dOSZLX69XatWuVmZmpBQsWqLW1\nVZLU2tqq+fPnKzs7W+vWrZPX6x3AwwAAIPz5jfYLL7ygRx99VGfPnj3v5319fSouLtbPf/5zbdu2\nTZWVlWpvb1d1dbV6e3tVWVmp3NxclZSUSJKKi4u1YsUKVVRUyLIs1dTUDM4RAQAQpvz+7fHRo0fr\n6aef1iOPPHLez48eParRo0crNjZWknTrrbdq3759amhoUGpqqiQpJSVFTU1NkqRDhw7p9ttvlyTN\nmDFDdXV1SktLG9CD8Wfnb1u07/BfgrrPa4HNFqH+fivUy7gmMOvgYM7BES5zvu3m63T/124Kyr78\nRnvWrFlqa2u74OddXV1yOj/5o+bR0dHq6upSV1eXHA6H7+c2m00ej0eWZSkiIsL33DNn/N8MJC4u\nKuC/TfuPf3j981FDZbNFBPR+uDjmGjzMOjiYc3CEw5w/HzX0sjf5GEif+S5fDodD3d3dvsfd3d1y\nOp0X/Nzr9cputysyMvK858bExPjdR6B3f4mPd553p7B7p47WvVNHB/SeuNA/zxmDh1kHB3MOjnCa\n80Aex6Dc5Wvs2LFqbW3Vhx9+qN7eXv3hD3/Ql770JbndbtXW1kqSGhoalJSUJEm65ZZbtHfvXklS\nbW2tpkyZ8ll3DQDANemKz7SrqqrU09OjzMxMrV69WosXL5ZlWZo7d65GjhyptLQ01dXVKSsrS5Zl\nqaioSJK0atUqrVmzRhs2bNCYMWM0a9asAT8YAADCWYRlWVfttwACvdwQTpdermbMOXiYdXAw5+Bg\nzhc3KJfHAQBAcBFtAAAMQbQBADAE0QYAwBBEGwAAQxBtAAAMQbQBADAE0QYAwBBEGwAAQxBtAAAM\nQbQBADAE0QYAwBBEGwAAQxBtAAAMQbQBADAE0QYAwBBEGwAAQxBtAAAMQbQBADAE0QYAwBBEGwAA\nQxBtAAAMQbQBADAE0QYAwBBEGwAAQxBtAAAMQbQBADAE0QYAwBBEGwAAQxBtAAAMQbQBADAE0QYA\nwBBEGwAAQxBtAAAMQbQBADAE0QYAwBBEGwAAQxBtAAAMQbQBADAE0QYAwBBEGwAAQxBtAAAMQbQB\nADAE0QYAwBBEGwAAQxBtAAAMQbQBADCE3d8TvF6vCgoKdOTIEQ0dOlTr169XQkKCb/vu3bu1ZcsW\nOZ1OpaenKyMjQ729vcrLy9N7770nh8OhtWvXKjExUc3NzVqyZIkSExMlSfPnz9c999wzaAcHAEA4\n8Rvt6upq9fb2qrKyUg0NDSopKVFZWZkkqaOjQ5s2bdKuXbsUExOjhQsXatq0aXrzzTcVFRWlnTt3\n6tixYyosLNSWLVt06NAh5eTkaNGiRYN+YAAAhBu/0d6/f79SU1MlSSkpKWpqavJta2tr0/jx4+Vy\nuSRJkyZNUmNjo1paWjRjxgxJ0pgxY3T06FFJUlNTk44fP66amholJCQoPz9fDodjwA8KAIBw5Dfa\nXV1d54XVZrPJ4/HIbrcrISFBLS0tam9vV3R0tOrr65WYmKgJEybojTfe0J133qnGxkadPHlS/f39\nmjx5sjIyMjRx4kSVlZVp8+bNWrVq1SX3HRcXJbvdFtABxsc7A3o9Ph3mHDzMOjiYc3Aw5yvjN9oO\nh0Pd3d2+x16vV3b7uZfFxsYqLy9Py5cvl8vlUnJysuLi4jRz5kwdPXpU2dnZcrvdSk5Ols1mU1pa\nmmJiYiRJaWlpKiwsvOy+Ozt7Ajk2xcc7derUmYDeA/4x5+Bh1sHBnIODOV/c5T7I+P32uNvtVm1t\nrSSpoaFBSUlJvm0ej0fNzc2qqKhQaWmpjh07JrfbrYMHD2ratGnasWOHZs+erVGjRkmSFi9erAMH\nDkiS6uvrlZycHNCBAQBwLfF7pp2Wlqa6ujplZWXJsiwVFRWpqqpKPT09yszMlCSlp6dr2LBhysnJ\n0fDhwyVJpaWlKi8vl9Pp1OOPPy5JKigoUGFhoYYMGaIRI0b4PdMGAACfiLAsywr1Ii4l0MsmXHoJ\nDuYcPMw6OJhzcDDniwvo8jgAALg6EG0AAAxBtAEAMATRBgDAEEQbAABDEG0AAAxBtAEAMATRBgDA\nEEQbAABDEG0AAAxBtAEAMATRBgDAEEQbAABDEG0AAAxBtAEAMATRBgDAEEQbAABDEG0AAAxBtAEA\nMATRBgDAEEQbAABDEG0AAAxBtAEAMATRBgDAEEQbAABDEG0AAAxBtAEAMATRBgDAEEQbAABDEG0A\nAAxBtAEAMATRBgDAEEQbAABDEG0AAAxBtAEAMATRBgDAEEQbAABDEG0AAAxBtAEAMATRBgDAEEQb\nAABDEG0AAAxBtAEAMATRBgDAEEQbAABDEG0AAAxh9/cEr9ergoICHTlyREOHDtX69euVkJDg2757\n925t2bJFTqdT6enpysjIUG9vr/Ly8vTee+/J4XBo7dq1SkxMVGtrq1avXq2IiAiNGzdO69atU2Qk\nnxsAAPg0/Bazurpavb29qqysVG5urkpKSnzbOjo6tGnTJm3btk3bt29XVVWV2tratHPnTkVFRWnn\nzp169NFHVVhYKEkqLi7WihUrVFFRIcuyVFNTM3hHBgBAmPEb7f379ys1NVWSlJKSoqamJt+2trY2\njR8/Xi6XS5GRkZo0aZIaGxvV0tKiGTNmSJLGjBmjo0ePSpIOHTqk22+/XZI0Y8YM7dmzZ8APCACA\ncOX38nhXV5ccDofvsc1mk8fjkd1uV0JCglpaWtTe3q7o6GjV19crMTFREyZM0BtvvKE777xTjY2N\nOnnypPr7+2VZliIiIiRJ0dHROnPmzGX3HRcXJbvdFtABxsc7A3o9Ph3mHDzMOjiYc3Aw5yvjN9oO\nh0Pd3d2+x16vV3b7uZfFxsYqLy9Py5cvl8vlUnJysuLi4jRz5kwdPXpU2dnZcrvdSk5Ols1mO+//\nr7u7uxUTE3PZfXd29nzW45J07pfh1KnLfzBA4Jhz8DDr4GDOwcGcL+5yH2T8Xh53u92qra2VJDU0\nNCgpKcm3zePxqLm5WRUVFSotLdWxY8fkdrt18OBBTZs2TTt27NDs2bM1atQoSdItt9yivXv3SpJq\na2s1ZcqUgA4MAIBrid8z7bS0NNXV1SkrK0uWZamoqEhVVVXq6elRZmamJCk9PV3Dhg1TTk6Ohg8f\nLkkqLS1VeXm5nE6nHn/8cUnSqlWrtGbNGm3YsEFjxozRrFmzBvHQAAAILxGWZVmhXsSlBHrZhEsv\nwcGcg4dZBwdzDg7mfHEBXR4HAABXB6INAIAhiDYAAIYg2gAAGIJoAwBgCKINAIAhiDYAAIYg2gAA\nGIJoAwBgCKINAIAhiDYAAIYg2gCAkNm2baseeugHWrbsu1q+fIkOH35XJ060KSPjPv3jrTE8Ho/m\nzbtXXV1dmj59ip58sui899m48UnNm3fveT/bt+/3Wrbsu1q27LuaOXOq79+HD7/7qda2bl2e+vr6\nLrk9P//hKzjSgeH3Ll8AAAyGlpYW1dXVqqxsiyIiIvSnPx3R+vUFeumlHfqXf7lB77yzX273uVs4\n/+53b8ntniKHw6HY2Fg1Nr4jj8cju92u/v5+vftu8wXvf9ttU3XbbVMlSffdN0vPPPP8Fa3vsceK\nL7u9qOjJK3q/gUC0AQDa+dsW7Tv8lwF9z9tuvk73f+2mS253Op06efIDvfrqb/TlL/93jRs3Xi+8\n8JIk6b77vqXXXnvVF+1XX31FDz74bUmSzWZXSsqt2rdvr6ZN+4r+679+r9tu+7Jee+3VT722BQvu\n16hRCRoyxK6lS1foqadK1Nt7VqdPt+s73/mBZsyYqXnz7tUvfvG/9NRTxRoyZIg++OB9nT7drvz8\nAo0ff7Puu2+WXnnldS1b9l2NGzdex44dVU9PlwoLn9D1139RW7e+qNraN+Ryxelvf/ubvv3t7/mO\n57Pi8jgAICRGjhypkpINOnCgUUuW5Cg7e6727HlbkjRjxh1qaPi/Onv2b2pvb9fp06c1ceIk32vT\n0marpub/SJKqq1/TXXfNvqJ9f/zxx1q4cLEee6xYra3/T1lZ/0MbNz6rRx75sXbt2nnB86+//ova\nsOEZzZ2bqVde2XXB9gkTklVa+qymTPmy/vM/X9ef/vRH/f73e/TCC/9TxcVP6fTp9ita36Vwpg0A\n0P1fu+myZ8WDobW1VdHR0crPXydJOny4WT/60Q/ldk9RTEysUlNnqrb2TX3wwQf6+tfvO++1kyf/\nN23YUKK//vVD/fWvf9XIkV+84v2PHp0oSfrCF0bopZe26NVXfyMpQh6P54Lnjhs3XpJ03XUjdfBg\n4wXbk5LObR85cqROnz6t1tbjmjAhWTabTTabTTffPOGK13cxnGkDAELiyJEj2rDhp74ve40aNVoO\nh1ORkTZJ0r33fkvV1a/r7bff1KxZ95z32oiICE2d+hU99VSJUlNnfqb9R0RESJJefLFcs2d/XWvW\nFF7y8vXfn+vvvf7uxhvH6vDhQ/J6vert7dUf/3jkM63xn3GmDQAIibvuuksHDjTr299+QFFRn5fX\na+kHP3hIDodDkpSYeKM+/vhjJSbe6PvZ+a+/W9/5zgN6+OH8gNZxxx3/ps2bS7V9+1bFx1+nDz/8\nMKD3k6SxY2/S1Klf0ZIlCxUb65LdbpfdHnhyI6x//E79VebUqTMBvT4+3hnwe8A/5hw8zDo4mHNw\nhPOcOzs79MYbNZozJ0O9vb1asOB+lZaW6/rrr/f72vh45yW3caYNAMAAi4116fDhc1cRIiKkb3zj\nW58q2P4QbQAABlhkZKTvC3YD+r4D/o4AAGBQEG0AAAxBtAEAMATRBgDAEEQbABAyV/Ndvv7uV7+6\n8M+ahgrfHgcAhMTVfpcv6dyHhe3bt2ru3PsDOdQBQ7QBANrV8h965y8HB/Q9v3TdJM256RuX3B7K\nu3zt379PL75YJpvNrhtuGKUf/ShPJ060qbj4J7Lb7bIsS489VqRXXvm1PvywUz/72U+1cuUjAUxj\nYHB5HAAQEqG6y5fX69WTTxapuHiDnnnmeblccXr99f+tvXvrNXHiZG3c+Kxycr6jjz76SA88sEgu\nV9xVEWyJM20AgKQ5N33jsmfFgyFUd/nq6Ditjo4OPfrouRCfPfs3DRkyRP/+7w9q+/aXlJu7XA6H\nQ0uWLBu4gx0gnGkDAEIiVHf5iosbrvj4eP30pz/TM888rwULFsntnqK33npTbvcUlZaWKTV1pnbs\n2KbIyEh5vd4BOd6BwJk2ACAkQnWXL5vNpmXLVio394eyLEvR0Q6tWfMTfeELI1Rc/BMNGTJEXq9X\nDz2Uq8jISN1wwyg9/niBfvzjgoE47IBwly8EjDkHD7MODuYcHMz54i53ly8ujwMAYAiiDQCAIYg2\nAACGINoAABiCaAMAYAiiDQCAIYg2AACGINoAABiCaAMAYAiiDQCAIYg2AACGINoAABiCaAMAYAi/\nt+b0er0qKCjQkSNHNHToUK1fv14JCQm+7bt379aWLVvkdDqVnp6ujIwM9fX1afXq1Tpx4oQiIyNV\nWFiosWPHqrm5WUuWLFFiYqIkaf78+brnnnsusWcAAPCP/Ea7urpavb29qqysVENDg0pKSlRWViZJ\n6ujo0KZNm7Rr1y7FxMRo4cKFmjZtmg4fPiyPx6OXX35ZdXV12rhxo55++mkdOnRIOTk5WrRo0aAf\nGAAA4cZvtPfv36/U1FRJUkpKipqamnzb2traNH78eLlcLknSpEmT1NjYqJtvvln9/f3yer3q6uqS\n3X5uN01NTTp+/LhqamqUkJCg/Pz8i97YHAAAXMhvtLu6us4Lq81mk8fjkd1uV0JCglpaWtTe3q7o\n6GjV19crMTFRUVFROnHihO6++251dnaqvLxckjR58mRlZGRo4sSJKisr0+bNm7Vq1apL7jsuLkp2\nuy2gA7zczcQxcJhz8DDr4GDOwcGcr4zfaDscDnV3d/see71e35lzbGys8vLytHz5crlcLiUnJysu\nLk5bt27V9OnTlZubq/fff18PPvigqqqqlJaWppiYGElSWlqaCgsLL7vvzs6eQI5N8fFOnTp1JqD3\ngH/MOXiYdXAw5+Bgzhd3uQ8yfr897na7VVtbK0lqaGhQUlKSb5vH41Fzc7MqKipUWlqqY8eOye12\nKyYmRk7nuZ3GxsbK4/Gov79fixcv1oEDByRJ9fX1Sk5ODujAAAC4lvg9005LS1NdXZ2ysrJkWZaK\niopUVVWlnp4eZWZmSpLS09M1bNgw5eTkaPjw4Vq4cKHy8/OVnZ2tvr4+rVy5UlFRUSooKFBhYaGG\nDBmiESNG+D3TBgAAn4iwLMsK9SIuJdDLJlx6CQ7mHDzMOjiYc3Aw54sL6PI4AAC4OhBtAAAMQbQB\nADAE0QYAwBBEGwAAQxBtAAAMQbQBADAE0QYAwBBEGwAAQxBtAAAMQbQBADAE0QYAwBBEGwAAQxBt\nAAAMQbQBADAE0QYAwBBEGwAAQxBtAAAMQbQBADAE0QYAwBBEGwAAQxBtAAAMQbQBADAE0QYAwBBE\nGwAAQxBtAAAMQbQBADAE0QYAwBBEGwAAQxBtAAAMQbQBADAE0QYAwBBEGwAAQxBtAAAMQbQBADAE\n0QYAwBBEGwAAQxBtAAAMQbQBADAE0QYAwBBEGwAAQxBtAAAMQbQBADAE0QYAwBBEGwAAQxBtAAAM\nQbQBADCE3d8TvF6vCgoKdOTIEQ0dOlTr169XQkKCb/vu3bu1ZcsWOZ1OpaenKyMjQ319fVq9erVO\nnDihyMhIFRYWauzYsWptbdXq1asVERGhcePGad26dYqM5HMDAACfht9iVldXq7e3V5WVlcrNzVVJ\nSYlvW0dHhzZt2qRt27Zp+/btqqqqUltbm9566y15PB69/PLLWrp0qTZu3ChJKi4u1ooVK1RRUSHL\nslRTUzN4RwYAQJjxG+39+/crNTVVkpSSkqKmpibftra2No0fP14ul0uRkZGaNGmSGhsbdeONN6q/\nv19er1ddXV2y28+d0B86dEi33367JGnGjBnas2fPYBwTAABhye/l8a6uLjkcDt9jm80mj8cju92u\nhIQEtbS0qL29XdHR0aqvr1diYqKioqJ04sQJ3X333ers7FR5ebkkybIsRURESJKio6N15syZy+47\nLi5KdrstkONTfLwzoNfj02HOwcOsg4M5BwdzvjJ+o+1wONTd3e177PV6fWfOsbGxysvL0/Lly+Vy\nuZScnKy4uDht3bpV06dPV25urt5//309+OCDqqqqOu//r7u7uxUTE3PZfXd29nzW45J07pfh1KnL\nfzBA4Jhz8DDr4GDOwcGcL+5yH2T8Xh53u92qra2VJDU0NCgpKcm3zePxqLm5WRUVFSotLdWxY8fk\ndrsVExMjp/PcTmNjY+XxeNTf369bbrlFe/fulSTV1tZqypQpAR0YAADXEr9n2mlpaaqrq1NWVpYs\ny1JRUZGqqqrU09OjzMxMSVJ6erqGDRumnJwcDR8+XAsXLlR+fr6ys7PV19enlStXKioqSqtWrdKa\nNWu0YcMGjRkzRrNmzRr0AwQAIFxEWJZlhXoRlxLoZRMuvQQHcw4eZh0czDk4mPPFBXR5HAAAXB2I\nNgAAhiDaAAAYgmgDAGAIog0AgCGINgAAhiDaAAAYgmgDAGAIog0AgCGINgAAhiDaAAAYgmgDAGAI\nog0AgCGINgAAhiDaAAAYgmgDAGAIog0AgCGINgAAhiDaAAAYgmgDAGAIog0AgCGINgAAhiDaAAAY\ngmgDAGAIog0AgCGINgAAhiDaAAAYgmgDAGAIog0AgCGINgAAhiDaAAAYgmgDAGAIog0AgCGINgAA\nhiDaAAAYgmgDAGAIog0AgCGINgAAhiDaAAAYgmgDAGAIog0AgCGINgAAhiDaAAAYgmgDAGAIog0A\ngCGINgAAhrD7e4LX61VBQYGOHDmioUOHav369UpISPBt3717t7Zs2SKn06n09HRlZGRo165d+vWv\nfy1JOnv2rN59913V1dWpra1NS5YsUWJioiRp/vz5uueeewbnyAAACDN+o11dXa3e3l5VVlaqoaFB\nJSUlKisrkyR1dHRo06ZN2rVrl2JiYrRw4UJNmzZNc+bM0Zw5cyRJjz32mObOnauYmBgdOnRIOTk5\nWrRo0eAeFQAAYchvtPfv36/U1FRJUkpKipqamnzb2traNH78eLlcLknSpEmT1NjYqBtuuEGSdPDg\nQbW0tGjdunWSpKamJh0/flw1NTVKSEhQfn6+HA7HgB/Upexq+Q+985eDQdvftcIWGaF+rxXqZVwT\nmHVwMOfgCJc5f+m6SZpz0zeCsi+/0e7q6jovrDabTR6PR3a7XQkJCWppaVF7e7uio6NVX1/vu/Qt\nSc8995yWLl3qezx58mRlZGRo4sSJKisr0+bNm7Vq1apL7jsuLkp2u+0zHto58fFO37+jTgyVLTIi\noPfDxTHX4GHWwcGcgyMc5hz1+aHntWYw+Y22w+FQd3e377HX65Xdfu5lsbGxysvL0/Lly+VyuZSc\nnKy4uDhJ0kcffaTjx49r6tSpvtempaUpJibG9+/CwsLL7ruzs+fKj+gfxMc7derUGd/j2f96l2b/\n610BvScu9M9zxuBh1sHBnIMjnOY8kMdxuQ8Afr897na7VVtbK0lqaGhQUlKSb5vH41Fzc7MqKipU\nWlqqY8eOye12S5L27dunadOmnfdeixcv1oEDByRJ9fX1Sk5OvvKjAQDgGuX3TDstLU11dXXKysqS\nZVkqKipSVVWVenp6lJmZKUlKT0/XsGHDlJOTo+HDh0uSjh8/7vu/7b8rKChQYWGhhgwZohEjRvg9\n0wYAAJ+IsCzrqv0WQKCXG8Lp0svVjDkHD7MODuYcHMz54gK6PA4AAK4ORBsAAEMQbQAADEG0AQAw\nBNEGAMAQRBsAAEMQbQAADEG0AQAwBNEGAMAQV/VfRAMAAJ/gTBsAAEMQbQAADEG0AQAwBNEGAMAQ\nRBsAAEMQbQAADBGW0fZ6vVq7dq0yMzO1YMECtba2hnpJYamvr08PP/ywsrOzNW/ePNXU1IR6SWHt\n9OnT+upXv6qjR4+Geilh67nnnlNmZqbmzJmjX/7yl6FeTtjq6+tTbm6usrKylJ2dze/0FQjLaFdX\nV6u3t1eVlZXKzc1VSUlJqJcUll555RW5XC5VVFToxRdfVGFhYaiXFLb6+vq0du1afe5znwv1UsLW\n3r179c4772jHjh3atm2bPvjgg1AvKWy99dZb8ng8evnll7V06VJt3Lgx1EsyRlhGe//+/UpNTZUk\npaSkqKmpKcQrCk+zZ8/WQw89JEmyLEs2my3EKwpfTzzxhLKysnTdddeFeilh63e/+52SkpK0dOlS\nfe9739PMmTNDvaSwdeONN6q/v19er1ddXV2y2+2hXpIxwnJSXV1dcjgcvsc2m00ej4dfjAEWHR0t\n6dy8f/jDH2rFihUhXlF42rVrl4YPH67U1FQ9//zzoV5O2Ors7NSf//xnlZeXq62tTd///vf12muv\nKSIiItRLCztRUVE6ceKE7r77bnV2dqq8vDzUSzJGWJ5pOxwOdXd3+x57vV6CPUjef/99PfDAA/rm\nN7+pe++9N9TLCUu/+tWvtGfPHi1YsEDvvvuuVq1apVOnToV6WWHH5XJp+vTpGjp0qMaMGaNhw4ap\no6Mj1MsKS1u3btX06dP1+uuv6ze/+Y1Wr16ts2fPhnpZRgjLaLvdbtXW1kqSGhoalJSUFOIVhaf2\n9nYtWrRIDz/8sObNmxfq5YStX/ziF9q+fbu2bdumCRMm6IknnlB8fHyolxV2br31Vr399tuyLEsn\nT57Uxx9/LJfLFeplhaWYmBg5nU5JUmxsrDwej/r7+0O8KjOE5elnWlqa6urqlJWVJcuyVFRUFOol\nhaXy8nJ99NFHevbZZ/Xss89Kkl544QW+LAUj3XHHHdq3b5/mzZsny7K0du1avqcxSBYuXKj8/Hxl\nZ2err69PK1euVFRUVKiXZQTu8gUAgCHC8vI4AADhiGgDAGAIog0AgCGINgAAhiDaAAAYgmgDAGAI\nog0AgCGINgAAhvj/3oqZzgMXDBkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe2e5fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(n, scores_train_svc, label='SVM Training')\n",
    "plt.plot(n, scores_test_svc, label = 'SVM Test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Train a non-linear SVM with soft margin and Gaussian kernel. Vary both the soft margin parameter and the Gaussian kernel bandwidth (sigma) and plot the classification error as a function of the margin parameter and kernel bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_usps_data = scaler.fit_transform(usps_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_traing, X_testg, y_traing, y_testg = train_test_split(scaled_usps_data, usps_labels, test_size=0.2, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma =  {'1': 0.0001, '2':0.02, '3':0.01, '4':0.1, '5':0.05, '6':1}\n",
    "gamma_vals = {'g1':1/sigma['1'], 'g2': 1/sigma['2'], 'g3':1/sigma['3'], 'g4':1/sigma['4'], 'g5':1/sigma['5']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.025000000000000022, 0.025000000000000022, 0.025000000000000022, 0.025000000000000022, 0.025000000000000022, 0.025000000000000022, 0.025000000000000022, 0.025000000000000022, 0.025000000000000022, 0.025000000000000022]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "resubstitution_svc1 = []\n",
    "generalization_svc1 = []\n",
    "\n",
    "for i in np.arange(0.1, 10, 1):\n",
    "    \n",
    "    gclf = SVC(kernel='rbf', C=i, gamma=gamma_vals['g1'])\n",
    "    gclf.fit(X_traing, y_traing.values.ravel())\n",
    "    #clf.predict(X_test)\n",
    "    \n",
    "    gy_val_train = y_traing\n",
    "    gy_pred_train = gclf.predict(X_traing)\n",
    "    \n",
    "    gy_val = y_testg\n",
    "    gy_pred = gclf.predict(X_testg)\n",
    "    \n",
    "    gscores_train = accuracy_score(y_val_train, y_pred_train)\n",
    "    training_error = 1 - gscores_train\n",
    "    resubstitution_svc1.append(training_error)\n",
    "    \n",
    "    gscores_test = accuracy_score(y_val, y_pred)\n",
    "    test_error = 1 - gscores_test\n",
    "    generalization_svc1.append(test_error)\n",
    "\n",
    "print(resubstitution_svc1)\n",
    "print(generalization_svc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.025000000000000022, 0.025000000000000022, 0.025000000000000022, 0.025000000000000022, 0.025000000000000022, 0.025000000000000022, 0.025000000000000022, 0.025000000000000022, 0.025000000000000022, 0.025000000000000022]\n"
     ]
    }
   ],
   "source": [
    "resubstitution_svc2 = []\n",
    "generalization_svc2 = []\n",
    "\n",
    "for i in np.arange(0.1, 10, 1):\n",
    "    \n",
    "    gclf = SVC(kernel='rbf', C=i, gamma=gamma_vals['g2'])\n",
    "    gclf.fit(X_traing, y_traing.values.ravel())\n",
    "    #clf.predict(X_test)\n",
    "    \n",
    "    gy_val_train = y_traing\n",
    "    gy_pred_train = gclf.predict(X_traing)\n",
    "    \n",
    "    gy_val = y_testg\n",
    "    gy_pred = gclf.predict(X_testg)\n",
    "    \n",
    "    gscores_train = accuracy_score(y_val_train, y_pred_train)\n",
    "    training_error1 = 1 - gscores_train\n",
    "    resubstitution_svc2.append(training_error1)\n",
    "    \n",
    "    gscores_test = accuracy_score(y_val, y_pred)\n",
    "    test_error1 = 1 - gscores_test\n",
    "    generalization_svc2.append(test_error1)\n",
    "\n",
    "print(resubstitution_svc2)\n",
    "print(generalization_svc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.025000000000000022, 0.025000000000000022, 0.025000000000000022, 0.025000000000000022, 0.025000000000000022, 0.025000000000000022, 0.025000000000000022, 0.025000000000000022, 0.025000000000000022, 0.025000000000000022]\n"
     ]
    }
   ],
   "source": [
    "resubstitution_svc5 = []\n",
    "generalization_svc5 = []\n",
    "\n",
    "for i in np.arange(0.1, 10, 1):\n",
    "    \n",
    "    gclf = SVC(kernel='rbf', C=i, gamma=gamma_vals['g5'])\n",
    "    gclf.fit(X_traing, y_traing)\n",
    "    #clf.predict(X_test)\n",
    "    \n",
    "    gy_val_train = y_traing\n",
    "    gy_pred_train = gclf.predict(X_traing)\n",
    "    \n",
    "    gy_val = y_testg\n",
    "    gy_pred = gclf.predict(X_testg)\n",
    "    \n",
    "    gscores_train = accuracy_score(y_val_train, y_pred_train)\n",
    "    training_error2 = 1 - gscores_train\n",
    "    resubstitution_svc5.append(training_error2)\n",
    "    \n",
    "    gscores_test = accuracy_score(y_val, y_pred)\n",
    "    test_error2 = 1 - gscores_test\n",
    "    generalization_svc5.append(test_error2)\n",
    "\n",
    "print(resubstitution_svc5)\n",
    "print(generalization_svc5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0xf84beb8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqcAAADdCAYAAACGw9uKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2m5M7uwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf48ba20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g_range = np.arange(0.1, 10, 1)\n",
    "fig1 = plt.figure(figsize=[15,15])\n",
    "n1 = fig1.add_subplot(4, 4, 1)\n",
    "n2 = fig1.add_subplot(4, 4, 2)\n",
    "n3 = fig1.add_subplot(4, 4, 3)\n",
    "\n",
    "n1.plot(g_range, resubstitution_svc1,label=\"resubstitution error\")\n",
    "n1.plot(g_range, generalization_svc1,label=\"generalization_error\")\n",
    "n1.set_xlabel('range')\n",
    "n1.set_ylabel('Error rate')\n",
    "n1.legend()\n",
    "\n",
    "n2.plot(g_range, resubstitution_svc2,label=\"resubstitution error\")\n",
    "n2.plot(g_range, generalization_svc2,label=\"generalization_error\")\n",
    "n2.set_xlabel('range')\n",
    "n2.set_ylabel('Error rate')\n",
    "n2.legend()\n",
    "\n",
    "n3.plot(g_range, resubstitution_svc5,label=\"resubstitution error\")\n",
    "n3.plot(g_range, generalization_svc5,label=\"generalization_error\")\n",
    "n3.set_xlabel('range')\n",
    "n3.set_ylabel('Error rate')\n",
    "n3.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the best parameters for the Classifier systematically with Grid Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'C': 1.0000100000000001, 'gamma': 1.0000000000000001e-05} with a score of 0.91\n"
     ]
    }
   ],
   "source": [
    "from matplotlib.colors import Normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "C_range = np.arange(0.00001, 10, 1)\n",
    "gamma_range = np.arange(0.00001, 10, 1)\n",
    "grid = dict(gamma=gamma_range, C=C_range)\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(svm.SVC(), param_grid=grid, cv=cv)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print('The best parameters are %s with a score of %0.2f'\n",
    "     %(grid.best_params_, grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gammas = np.logspace(-2, 1, 4)\n",
    "cvals = np.logspace(-2, 1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "   \n",
    "    ('svc', SVC()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_scores, test_scores = calc_params(X_traing, y_traing, clf, gammas, 'svc__gamma', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. After you have selected parameter values for both algorithms (in part a. and part b.), and trained each one with the parameter value you have chosen, compute the classification error on the test set. Report the test set estimates of the error for both cases along with the parameter values you have selected, and compare the two results. Is a linear SVM a good choice for this data, or should we use a non-linear one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing accuracy for a Linear SVM with a 'C' value of 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.025000000000000022]\n"
     ]
    }
   ],
   "source": [
    "scores_test_svc = []\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "    #clf.predict(X_test)\n",
    "    \n",
    "y_val_train = y_train\n",
    "y_pred_train = clf.predict(X_train)\n",
    "    \n",
    "y_val = y_test\n",
    "y_pred = clf.predict(X_test)\n",
    "    \n",
    "scores_test = accuracy_score(y_val, y_pred)\n",
    "scores_n = 1 - scores_test\n",
    "scores_test_svc.append(scores_n)\n",
    "\n",
    "\n",
    "print(scores_test_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing accuracy of a Non-Linear SVM using the specified best parameters of 'C' = 1.0000100000000001 and Gamma = 1.0000000000000001e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.025000000000000022]\n"
     ]
    }
   ],
   "source": [
    "resubstitution_svc5 = []\n",
    "generalization_svc5 = []\n",
    "\n",
    "\n",
    "    \n",
    "gclf = SVC(kernel='rbf', C=1.0000100000001, gamma=1.000000000001e-05)\n",
    "gclf.fit(X_traing, y_traing)\n",
    "    #clf.predict(X_test)\n",
    "    \n",
    "gy_val = y_testg\n",
    "gy_pred = gclf.predict(X_testg)\n",
    "    \n",
    "gscores_test = accuracy_score(y_val, y_pred)\n",
    "test_error1 = 1 - gscores_test\n",
    "generalization_svc5.append(test_error1)\n",
    "\n",
    "\n",
    "print(generalization_svc5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. For each method (linear and Gaussian), select five correctly and five incorrectly classified samples. Plot these as images like the one above. (Hint: To do so, you will have to write a 256-vector into a 16x16 matrix (function imresize.m) and plot this matrix as an image functions imshow.m and colormap(gray)) Can you see a qualitative difference between correctly classified and misclassified digits? Explain the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFJCAYAAAASfw+VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGYtJREFUeJzt3XtwVPXdx/FPCCTcCXeogOVSKtcqVpGKInYolE6l49AC\naddanLYCA4UwVg0QGCygYwftMAOBdJiO4SJUmQEGaqvQQrlIKSCXQHEAAYPcAuGSEJJNcp4/eFwf\nqzn73d0fOYvP+/WXyflm90OyfnKS/H7npHie5wkA4KtO0AEA4E5AWQKAAWUJAAaUJQAYUJYAYEBZ\nAoBB3dp4kpSUFNPcwYMH1adPn9uc5pbGjRub5nbt2qX+/fv7zvTu3dtFJElSjx49THOzZ89WTk5O\njcdLSkpcRdLevXtNcxs3btTw4cN9Z06fPu0iksLhsGmuNl9TsUjGXGSS/FZSJtWZpcvScaVnz55B\nR/hSHTp0CDrCF3Tv3j3oCF+QjK8pKTlzkclfUpUlACQryhIADChLADCgLAHAIK6/hldXV2vWrFk6\nevSo0tLS9Lvf/U53332362wAkDTiOrN87733VFFRoVWrVmnq1Kl6+eWXXecCgKQSV1nu2bNHjzzy\niCTp3nvv1aFDh5yGAoBkE9eP4SUlJZ9b1J2amqrKykrVrfvlD3fw4EHzeqlkvLzm9evXg47wpZYu\nXRp0hC84duxY0BG+IBlfU1Jy5iJTzeIqy8aNG6u0tDTydnV1dY1FKcm8At/zPPNun0RZd/Bcv35d\nTZo08Z0JYgfP0qVLNXbs2BqPB7GD59ixY+rWrZvvTG3v4KnN11QskjEXmW7DDp5+/fpp69atkqQP\nPvggKXduAIBLcZ1ZDhkyRNu3b9fo0aPleZ7mzp3rOhcAJJW4yrJOnTqaPXu26ywAkLRYlA4ABpQl\nABhQlgBgQFkCgAFlCQAGtXJbiWRUr149Z7Nt2rRJNE5ELFdm95u9fPmyiziSpBMnTphnMzIyfI+f\nOXMm0TiS7IvSAVc4swQAA8oSAAwoSwAwoCwBwICyBAADyhIADChLADCgLAHAgLIEAAPKEgAMKEsA\nMKAsAcCAsgQAA8oSAAwoSwAwoCwBwICyBACD/7dXSq9Tx/59Itps/fr1E40T0bRpUyezN2/edBFH\nkturyqekpCQaBwgEZ5YAYEBZAoABZQkABpQlABhQlgBgENdfw8PhsLKzs3XmzBlVVFRo3Lhx+u53\nv+s6GwAkjbjKct26dcrIyNCrr76qK1eu6Ec/+hFlCeArLa6yHDZsmIYOHSpJ8jxPqampTkMBQLKJ\nqywbNWokSSopKdGkSZM0efJkp6EAINmkeJ7nxfOBZ8+e1YQJE5SZmamRI0f6zh46dEi9e/eOKyAA\nJIO4yrKoqEihUEg5OTkaMGBA9CcxbnHzPK/WtsO1bNnSNFdUVKRWrVr5zrj8fa31sX71q19pyZIl\nNR6/cOGCq0jasGGDaW7nzp1RXw/79+93EUllZWWmudp8TcUiGXOR6dbz1SSupUO5ubm6du2aFi5c\nqFAopFAo5HQvMgAkm7h+Zzl9+nRNnz7ddRYASFosSgcAA8oSAAwoSwAwoCwBwICyBAADbivhYDaW\n2y5EE8vWUb/ZOPcafKnKykpns9XV1YnGAQLBmSUAGFCWAGBAWQKAAWUJAAaUJQAYUJYAYEBZAoAB\nZQkABpQlABhQlgBgQFkCgAFlCQAGlCUAGFCWAGBAWQKAAWUJAAaUJQAYUJYAYHDH3VYiJSXFyeOk\npaU5m23UqFGicSJiuUWF32x5ebmLOJKkmzdvOpv9qt9WwtVtQSQpPT090TiSYntNNWvWzPe4y6+f\n9TUa7f+/cDjsIk5UnFkCgAFlCQAGlCUAGFCWAGBAWQKAQUJleenSJQ0aNEjHjx93lQcAklLcZRkO\nh5WTk6P69eu7zAMASSnusnzllVc0evRotWnTxmUeAEhKKZ7nebF+0Jo1a3Tu3DmNHz9eoVBIs2bN\nUteuXWucP3TokHr37p1QUAAIUlxl+dOf/lQpKSlKSUnRkSNH9PWvf12LFi1S69atv/xJjLtuPM+L\nOutqB8/XvvY101xhYaE6dOjgO/ODH/zARSRJ0sMPP2yae+qpp/TGG2/UePzDDz90FUlr1641zR08\neFB9+vTxnTl69KiLSOZdG5bXlEvWHTyVlZWqW9d/A11t7+C5cuWKMjIyfGdqewdPeXl51M+Dyx08\nfv++uLY7Ll++PPLfn55Z1lSUAPBVwNIhADBI+EIa+fn5LnIAQFLjzBIADChLADCgLAHAgLIEAIM7\n7krpsVyJ2k+TJk2czTZv3jzROBF16ti/f/nNXr9+3UUcSVJJSYmz2aqqqkTjOBfLlcSjadGihXm2\nVatWvsc7duyYaBxJUtu2bc2zAwcO9D1eWlqaaJyIEydOmObatWvne/zcuXMu4kTFmSUAGFCWAGBA\nWQKAAWUJAAaUJQAYUJYAYEBZAoABZQkABpQlABhQlgBgQFkCgAFlCQAGlCUAGFCWAGBAWQKAAWUJ\nAAaUJQAYUJYAYHDH3VYiLS3NyeO0bNnS2WwstxKIpqKiwsnspUuXXMSRJN24ccPZrOd5icaRJNWt\na3/pRpuNdnuHWPTq1cs826dPH9/jDzzwQKJxJEmdO3c2z44YMcL3+Pnz5xONE5GSkmKa69q1q+/x\nK1euuIgTFWeWAGBAWQKAAWUJAAaUJQAYUJYAYBD3X8MXL16szZs3KxwOa8yYMfrxj3/sMhcAJJW4\nynLXrl3at2+fVq5cqbKyMi1dutR1LgBIKnGV5bZt29S9e3dNmDBBJSUl+u1vf+s6FwAklRQvjlXC\n06dP1yeffKLc3FwVFhZq3Lhxeuedd2pcZHro0CH17t074bAAEJS4ziwzMjLUpUsXpaWlqUuXLkpP\nT9fly5dr3OkSbafCpzzPi7qqv2HDhjHn/TL33XefaW7btm0aOHCg78wTTzzhIpIk+26SsWPH+v76\nY/Pmza4i6d133zXNnT9/Xm3btvWduXjxootISk1NNc2Fw2HVq1fPd6Z169YuIkmy7+B59913NWTI\nEN+Z2t7B88tf/lJ5eXm+My538Fheo5s3b9bjjz/uO7Nnzx5XkXT16tUaj8X11/D7779f//znP+V5\nns6fP6+ysjJlZGTEHRAAkl1cZ5aDBw/W7t27NXLkSHmep5ycHPN3egC4E8W9dIg/6gD4/4RF6QBg\nQFkCgAFlCQAGlCUAGFCWAGBwx91Won79+k4eJ5aFyNFmmzRpkmicCL9FsbHMulw8XFZW5my2Th03\n359jWdcbbfaee+5JNE5ELAvJo81+85vfTDSOpNhe63fddZfv8QYNGiQaJ6Jdu3ZO5tLT013EiYoz\nSwAwoCwBwICyBAADyhIADChLADCgLAHAgLIEAAPKEgAMKEsAMKAsAcCAsgQAA8oSAAwoSwAwoCwB\nwICyBAADyhIADChLADColSulx3J17Gizrq5K3rZtW2ezLq/U/PHHH5tnL168WOOxCxcuuIgjSaqo\nqHA227Bhw0TjSJK6devmbPbBBx9MNE7E3Xff7Wz2ypUricaRJJWWlppnT5486Xu8uro6wTSf8TzP\n6dztxpklABhQlgBgQFkCgAFlCQAGlCUAGFCWAGAQ19KhcDisF154QWfOnFGdOnX00ksvqWvXrq6z\nAUDSiOvMcsuWLaqsrNSbb76pCRMm6PXXX3edCwCSSlxl2blzZ1VVVam6ulolJSWqW7dW1rYDQGBS\nvDiWx589e1bjx4/XjRs3VFxcrNzcXPXr16/G+UOHDql3794JBQWAIMVVlvPmzVNaWpqmTp2qs2fP\n6uc//7nWr19f47a/1NRU0+NWVVVFne3YsWOscb/UsGHDTHO5ubl69tlnfWdcbpc7duyYaW7u3LnK\nzs6u8fiGDRtcRdLRo0dNczdv3lT9+vV9Z9LS0lxEUq9evUxzO3fu1IABA3xnBg0a5CKSpFs/dVn8\n+te/1uLFi31nbt686SKS6tWrZ5obP368Fi5c6Dvjcrvj9u3bo86sXLlSY8aM8Z3ZtGmTq0i+24Tj\n+vm5adOmkS9As2bNVFlZqaqqqvjSAcAdIK6yfPrpp5Wdna3MzEyFw2FNmTLF2QUSACAZxVWWjRo1\n0h/+8AfXWQAgabEoHQAMKEsAMKAsAcCAsgQAg1rZemNd52WZbd68eaJxJElt2rRxNuvysvfnzp1z\nMnv58mUXcSTZ18laZl2tk73//vudzX7jG99INE7EtWvXzLM3btzwPf7RRx8lGkdSbLfyOH36tO/x\nBg0aJBonoqSkxMlcbS1b5MwSAAwoSwAwoCwBwICyBAADyhIADChLADCgLAHAgLIEAAPKEgAMKEsA\nMKAsAcCAsgQAA8oSAAwoSwAwoCwBwICyBAADyhIADChLADColdtKpKWlOZtt1apVonEkxXZ7imiz\nZWVlicaJKCoqcjJbXl7uIo4kqWXLls5m+/btm2gcSVKvXr2czbq8LUi02zLEMmu97UI0jRs3Ns/W\nqeN//nT9+vVE40RYb30Sba6iosJFnKg4swQAA8oSAAwoSwAwoCwBwICyBAADU1nu379foVBIknTq\n1CmNGTNGmZmZmjlzpqqrq29rQABIBlHLMi8vT9OnT48sRZk3b54mT56sFStWyPM8bdq06baHBICg\nRS3LTp06acGCBZG3CwoK9OCDD0qSHn30Ue3YseP2pQOAJJHiGVbkFhYWKisrS6tXr9bAgQO1bds2\nSdLOnTv19ttv6/e//73vxx8+fFg9e/Z0kxgAAhDzDp7/u8K/tLRUTZs2jfoxDz30kOmxr127FvXx\n+vfvb3qsaIYPH26amzJlil577TXfmXr16rmIJEn629/+Zppbt26dnnjiiRqPv//++64iqX79+qa5\n06dPq1OnTr4zDz/8sItIevTRR01z48aN06JFi3xnXH79CgoKTHOvvfaapkyZ4jtz9epVF5HUrl07\n09zcuXOVnZ3tO+Nyt8zOnTujzmzfvj3qa+bAgQOuIvnuUIr5r+E9e/bUrl27JElbt27Vt7/97fiT\nAcAdIuayfP7557VgwQKNGjVK4XBYQ4cOvR25ACCpmH4M79Chg1avXi1J6ty5s5YtW3ZbQwFAsmFR\nOgAYUJYAYEBZAoABZQkABpQlABjccbeViOV2EH5iudR+tNni4uJE40TEshDZbzY9Pd1FHElSt27d\nnM1+61vfSjSOJKlJkybOZk+ePJlgms+UlpY6m3V1C5WGDRs6m/3kk08SjRNx4cIFJ3PcVgIAkghl\nCQAGlCUAGFCWAGBAWQKAAWUJAAaUJQAYUJYAYEBZAoABZQkABpQlABhQlgBgQFkCgAFlCQAGlCUA\nGFCWAGBAWQKAQa1cKb1uXfvTRJuN5Qrnfho0aOBs9tKlS4nGca5Dhw7OHqt///7OZu+5555E40iK\n7erY0a6+H8vrM5pYPu/RZl1d7f78+fPm2cuXL/seP3bsWKJxIoqKipzMhcNhF3Gi4swSAAwoSwAw\noCwBwICyBAADyhIADExluX//foVCIUnSkSNHlJmZqVAopGeeecb8Fy0AuJNFLcu8vDxNnz5d5eXl\nkqQ5c+ZoxowZys/P15AhQ5SXl3fbQwJA0KKWZadOnbRgwYLI2/Pnz1ePHj0kSVVVVc7WggFAMkvx\nPM+LNlRYWKisrCytXr068r69e/dq2rRpWr58uVq0aOH78f/5z3+cLUYGgCDEtXVh48aNWrRokZYs\nWRK1KCXpscceMz3uuXPn1K5dO9+Z4cOHmx4rmscff9w097Of/UzLli3znSksLHQRSZL0l7/8xTS3\nZcsWDRo0qMbjsexwicb69Zs3b55efPFF35lYdgP5sf77fvKTn3zum/yXOXHihItIkuy5cnJyNHv2\nbN+Z2t7BM3/+fGVlZfnO/Otf/3IRSZJUUFAQdaa4uFjNmzf3nbl69aqrSKqurq7xWMxluXbtWq1a\ntUr5+fnKyMhIKBgA3CliKsuqqirNmTNH7du318SJEyVJDzzwgCZNmnRbwgFAsjCVZYcOHSI/yrg8\nDQeAOwWL0gHAgLIEAAPKEgAMKEsAMKAsAcCgVm4rkZKS4mw2NTU10TiS3N5WolWrVonGifh0K2mi\ns5bNAlbf+973zLNDhw71Pd65c+dE40i6tYHB6u6773bynBaxXFgm2uvmo48+SjSOJOnAgQPOZo8f\nP55onIiSkhInc4ZNiE5wZgkABpQlABhQlgBgQFkCgAFlCQAGlCUAGFCWAGBAWQKAAWUJAAaUJQAY\nUJYAYEBZAoABZQkABpQlABhQlgBgQFkCgAFlCQAGlCUAGNTKbSUqKiqczRYXFycaR5J048YNZ7Md\nO3ZMNE5ELLeD8Lvdw1133eUijiTpvvvuM89+5zvf8T2elpaWaBxJUnl5uXm2efPmvsdPnjyZYJrP\nxHIriGizu3fvTjSOJOno0aPm2YKCAt/jly5dSjRORGVlpdO5240zSwAwoCwBwICyBAADyhIADChL\nADAwleX+/fsVCoU+977169dr1KhRtyUUACSbqEuH8vLytG7dOjVo0CDyvsOHD+utt96S53m3NRwA\nJIuoZ5adOnXSggULIm8XFxdr/vz5ys7Ovq3BACCZpHiG08PCwkJlZWVp5cqVmjhxoqZOnar09HRl\nZWVp9erVUZ/kyJEj6tGjh5PAABCEmHbwFBQU6NSpU5o1a5bKy8t17NgxzZkzR9OmTfP9uIEDB5oe\n/9KlS2rZsqXvzODBg815/YwYMcI0FwqFlJ+f7zvTpk0bF5EkSaWlpaa5J598UmvWrKnxeBA7eNLS\n0qLuwHK1g+fDDz80zXXv3j3q7L59+1xEkiT9+9//Ns29+uqreu6553xnansHz9mzZ9W+fXvfGZc7\neMLhcNQZz/OUkpLi7Dktz1eTmMqyb9++2rBhg6TPzjajFSUAfBWwdAgADExl2aFDhy/8bvLL3gcA\nX1WcWQKAAWUJAAaUJQAYUJYAYFArV0ovKytzNnv8+PFE40iS9u7da5oLhUJRZ2O5kng0sazZbNiw\nYY3HUlNTXcSRJH388cemua5du0advXr1qotIOnDggGmue/fu2rFjh+/M+++/7yKSJGnPnj3m2X/8\n4x++x2O56rqfWD7nRUVFvseT5arlQeDMEgAMKEsAMKAsAcCAsgQAA8oSAAwoSwAwoCwBwICyBAAD\nyhIADChLADCgLAHAgLIEAAPKEgAMKEsAMKAsAcCAsgQAA8oSAAwoSwAwSPE8zws6BAAkO84sAcCA\nsgQAA8oSAAwoSwAwoCwBwICyBACDwMuyurpaOTk5GjVqlEKhkE6dOhV0JElSOBzWc889p8zMTI0c\nOVKbNm0KOlLEpUuXNGjQIB0/fjzoKJKkxYsXa9SoUXryySf15z//Oeg4km59/aZOnarRo0crMzMz\n8M/V/v37FQqFJEmnTp3SmDFjlJmZqZkzZ6q6ujopch05ckSZmZkKhUJ65plnVFRUFHimT61fv16j\nRo0KJM+nAi/L9957TxUVFVq1apWmTp2ql19+OehIkqR169YpIyNDK1as0B//+Ee99NJLQUeSdKsE\ncnJyVL9+/aCjSJJ27dqlffv2aeXKlcrPz9e5c+eCjiRJ2rJliyorK/Xmm29qwoQJev311wPLkpeX\np+nTp6u8vFySNG/ePE2ePFkrVqyQ53mBfSP+71xz5szRjBkzlJ+fryFDhigvLy/wTJJ0+PBhvfXW\nWwp6SXjgZblnzx498sgjkqR7771Xhw4dCjjRLcOGDdNvfvMbSZLneUpNTQ040S2vvPKKRo8erTZt\n2gQdRZK0bds2de/eXRMmTNCzzz6rxx57LOhIkqTOnTurqqpK1dXVKikpUd26dQPL0qlTJy1YsCDy\ndkFBgR588EFJ0qOPPqodO3YkRa758+erR48ekqSqqiqlp6cHnqm4uFjz589XdnZ2rWf5b8G9gv5X\nSUmJGjduHHk7NTVVlZWVgb64JalRo0aSbuWbNGmSJk+eHGgeSVqzZo1atGihRx55REuWLAk6jqRb\nL+ZPPvlEubm5Kiws1Lhx4/TOO+8oJSUl0FwNGzbUmTNn9P3vf1/FxcXKzc0NLMvQoUNVWFgYedvz\nvMjnp1GjRrp+/XpS5Pr0G/DevXu1bNkyLV++PNBMVVVVmjZtml588cVAivu/BX5m2bhxY5WWlkbe\nrq6uDrwoP3X27Fk99dRTGjFihH74wx8GHUdvv/22duzYoVAopCNHjuj555/XxYsXA82UkZGhgQMH\nKi0tTV26dFF6erouX74caCZJ+tOf/qSBAwfqr3/9q9auXasXXnjhcz/aBalOnc/+tystLVXTpk0D\nTPN5Gzdu1MyZM7VkyRK1aNEi0CwFBQU6deqUZs2apaysLB07dkxz5swJLE/grdSvXz/9/e9/1/Dh\nw/XBBx+oe/fuQUeSJBUVFWns2LHKycnRgAEDgo4jSZ/7Th8KhTRr1iy1bt06wETS/fffrzfeeEO/\n+MUvdOHCBZWVlSkjIyPQTJLUtGlT1atXT5LUrFkzVVZWqqqqKuBUt/Ts2VO7du1S//79tXXrVj30\n0ENBR5IkrV27VqtWrVJ+fn5SfA379u2rDRs2SJIKCwuVlZWladOmBZYn8LIcMmSItm/frtGjR8vz\nPM2dOzfoSJKk3NxcXbt2TQsXLtTChQsl3frlc7L8YSVZDB48WLt379bIkSPleZ5ycnKS4ve7Tz/9\ntLKzs5WZmalwOKwpU6aoYcOGQceSJD3//POaMWOG5s+fry5dumjo0KFBR1JVVZXmzJmj9u3ba+LE\niZKkBx54QJMmTQo4WfLgqkMAYBD47ywB4E5AWQKAAWUJAAaUJQAYUJYAYEBZAoABZQkABpQlABj8\nD8SlOQFmbUB8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe2e5940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "row1 = X_train[0:1]\n",
    "row1 = row1.values\n",
    "M = row1.reshape(16,16)\n",
    "\n",
    "imgplot = plt.imshow(M, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "#### (E-commerce Customer Identification using ensemble of classifiers)\n",
    "\n",
    "#### The data of e-tailer customers is posted under the course documents for week 6. The training data contains 334 variables for a known set of 10,000 customers and non-customers with a ratio of 1:10, respectively. The test data consists of a set of examples and is drawn from the same distribution as the training set.  The feature data is train10000.csv and the label data is train10000_label.csv with corresponding labels for the records in train10000.csv.  The test10000.csv is the test data with corresponding labels for the records in test10000-label.csv. \n",
    "\n",
    "#### Preprocessing steps to do:\n",
    "#### â€¢\tMissing values: Check if there is any missing values inside the dataset, if so, perform an analysis to determine if you can ignore the cases with missing data.  If you cannot ignore the missing data, fill in the missing values and explain the method that you used. \n",
    "#### â€¢\tNormalization:  since the features have very different value ranges, apply a normalization procedure to make the features comparable (be on the same scale).\n",
    "#### â€¢\tAttribute/Feature selection:  Since there are 334 features in the dataset, it may be useful to use some feature/attribute selection to reduce the dataset before training classifiers. Describe your selected method and explain how it works briefly.\n",
    "#### â€¢\tBalanced data: The dataset is a severely unbalanced dataset. You may want to balance the data before training the classifier. Describe your selected method to balance the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import Imputer\n",
    "import seaborn as sns\n",
    "import pylab\n",
    "\n",
    "\n",
    "data_train = pd.read_csv('train10000.csv', delimiter=',',na_values='nan', header=None)\n",
    "data_train_labels = pd.read_csv('train10000_Label.csv', delimiter=',',na_values='nan', header=None)\n",
    "data_test = pd.read_csv('test10000.csv', delimiter=',',na_values='nan', header=None)\n",
    "data_test_labels = pd.read_csv('test10000_label.csv', delimiter=',',na_values='nan', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Columns: 334 entries, 0 to 333\n",
      "dtypes: float64(13), int64(321)\n",
      "memory usage: 25.5 MB\n"
     ]
    }
   ],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>324</th>\n",
       "      <th>325</th>\n",
       "      <th>326</th>\n",
       "      <th>327</th>\n",
       "      <th>328</th>\n",
       "      <th>329</th>\n",
       "      <th>330</th>\n",
       "      <th>331</th>\n",
       "      <th>332</th>\n",
       "      <th>333</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>999000.0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>46</td>\n",
       "      <td>196000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>999000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>999000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>59</td>\n",
       "      <td>19</td>\n",
       "      <td>99</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>254</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50</td>\n",
       "      <td>196000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>51</td>\n",
       "      <td>24</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>169</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>999000.0</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50</td>\n",
       "      <td>196000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>999000.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>999000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61</td>\n",
       "      <td>20</td>\n",
       "      <td>98</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>337</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58</td>\n",
       "      <td>195000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>999000.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>19</td>\n",
       "      <td>83</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>280</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>999000.0</td>\n",
       "      <td>64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64</td>\n",
       "      <td>196000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>999000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>23</td>\n",
       "      <td>99</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>27</td>\n",
       "      <td>143</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 334 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3         4    5         6         7    8         9    \\\n",
       "0  999000.0   46  2.0   46  196000.0  2.0  999000.0       4.0  3.0  999000.0   \n",
       "1       1.0   50  1.0   50  196000.0  1.0       4.0       7.0  5.0       1.0   \n",
       "2  999000.0   50  1.0   50  196000.0  1.0       4.0  999000.0  7.0  999000.0   \n",
       "3       1.0   58  1.0   58  195000.0  2.0       4.0  999000.0  8.0       2.0   \n",
       "4  999000.0   64  1.0   64  196000.0  1.0       4.0       2.0  2.0  999000.0   \n",
       "\n",
       "  ...   324  325  326  327  328  329  330  331  332  333  \n",
       "0 ...    59   19   99   21    6    8   11   27  254  110  \n",
       "1 ...    51   24   89   13   11    2   10   30  169  144  \n",
       "2 ...    61   20   98   17    7    2   10   24  337  105  \n",
       "3 ...    50   19   83   12    8    4    4   31  280  165  \n",
       "4 ...    57   23   99   22    4    8   44   27  143   12  \n",
       "\n",
       "[5 rows x 334 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.99000000e+05,   4.60000000e+01,   2.00000000e+00, ...,\n",
       "          2.70000000e+01,   2.54000000e+02,   1.10000000e+02],\n",
       "       [  1.00000000e+00,   5.00000000e+01,   1.00000000e+00, ...,\n",
       "          3.00000000e+01,   1.69000000e+02,   1.44000000e+02],\n",
       "       [  9.99000000e+05,   5.00000000e+01,   1.00000000e+00, ...,\n",
       "          2.40000000e+01,   3.37000000e+02,   1.05000000e+02],\n",
       "       ..., \n",
       "       [  1.00000000e+00,   5.40000000e+01,   1.00000000e+00, ...,\n",
       "          3.20000000e+01,   3.31000000e+02,   1.14000000e+02],\n",
       "       [  9.99000000e+05,   4.40000000e+01,   1.00000000e+00, ...,\n",
       "          1.20000000e+01,   3.28000000e+02,   1.43000000e+02],\n",
       "       [  9.99000000e+05,   4.60000000e+01,   9.99000000e+05, ...,\n",
       "          1.80000000e+01,   2.23000000e+02,   1.83000000e+02]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 334)\n"
     ]
    }
   ],
   "source": [
    "rows, columns = data_train.shape\n",
    "print data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 334)\n"
     ]
    }
   ],
   "source": [
    "rows, columns = data_test.shape\n",
    "print data_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>missing</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>520579.2288</td>\n",
       "      <td>499079.648645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>999000.0</td>\n",
       "      <td>999000.00</td>\n",
       "      <td>999000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>46.6708</td>\n",
       "      <td>9.257170</td>\n",
       "      <td>18.0</td>\n",
       "      <td>46.00</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.00</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>243557.5701</td>\n",
       "      <td>428964.747884</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>999000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>46.8831</td>\n",
       "      <td>10.578271</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.00</td>\n",
       "      <td>46.0</td>\n",
       "      <td>52.00</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>196027.3000</td>\n",
       "      <td>950.497267</td>\n",
       "      <td>190000.0</td>\n",
       "      <td>196000.00</td>\n",
       "      <td>196000.0</td>\n",
       "      <td>196000.00</td>\n",
       "      <td>199000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>239960.8596</td>\n",
       "      <td>426798.361930</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>999000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>430870.8259</td>\n",
       "      <td>494785.452455</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>999000.00</td>\n",
       "      <td>999000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>706594.0515</td>\n",
       "      <td>454567.505278</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>999000.0</td>\n",
       "      <td>999000.00</td>\n",
       "      <td>999000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>302101.4378</td>\n",
       "      <td>458858.669779</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>999000.00</td>\n",
       "      <td>999000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>542557.5240</td>\n",
       "      <td>497664.991029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>999000.0</td>\n",
       "      <td>999000.00</td>\n",
       "      <td>999000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>52.8758</td>\n",
       "      <td>21.389131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>61.00</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>2.1090</td>\n",
       "      <td>1.132805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>769635.8644</td>\n",
       "      <td>420164.563919</td>\n",
       "      <td>10.0</td>\n",
       "      <td>999000.00</td>\n",
       "      <td>999000.0</td>\n",
       "      <td>999000.00</td>\n",
       "      <td>999000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>47.9219</td>\n",
       "      <td>33.206353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.00</td>\n",
       "      <td>42.0</td>\n",
       "      <td>73.00</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>520579.1544</td>\n",
       "      <td>499079.726258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>999000.0</td>\n",
       "      <td>999000.00</td>\n",
       "      <td>999000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>182.9210</td>\n",
       "      <td>107.950188</td>\n",
       "      <td>10.0</td>\n",
       "      <td>170.00</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.00</td>\n",
       "      <td>2100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>449.3776</td>\n",
       "      <td>39.947438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>428.00</td>\n",
       "      <td>449.0</td>\n",
       "      <td>471.00</td>\n",
       "      <td>811.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>482.3560</td>\n",
       "      <td>36.297088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>462.00</td>\n",
       "      <td>482.0</td>\n",
       "      <td>503.00</td>\n",
       "      <td>811.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>358.7217</td>\n",
       "      <td>40.975172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>334.00</td>\n",
       "      <td>356.0</td>\n",
       "      <td>380.00</td>\n",
       "      <td>811.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>5.8874</td>\n",
       "      <td>2.376149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>702.7789</td>\n",
       "      <td>248.323896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>528.00</td>\n",
       "      <td>650.0</td>\n",
       "      <td>822.25</td>\n",
       "      <td>2162.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>313.2410</td>\n",
       "      <td>29.777753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300.00</td>\n",
       "      <td>310.0</td>\n",
       "      <td>330.00</td>\n",
       "      <td>660.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>619.9018</td>\n",
       "      <td>213.735578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>467.00</td>\n",
       "      <td>580.0</td>\n",
       "      <td>725.00</td>\n",
       "      <td>2022.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>262.1950</td>\n",
       "      <td>39.025687</td>\n",
       "      <td>0.0</td>\n",
       "      <td>240.00</td>\n",
       "      <td>260.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>630.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>1810.8304</td>\n",
       "      <td>1134.881939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1114.00</td>\n",
       "      <td>1513.0</td>\n",
       "      <td>2136.00</td>\n",
       "      <td>11839.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>4.4971</td>\n",
       "      <td>1.126818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>54.5342</td>\n",
       "      <td>8.657131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>55.0</td>\n",
       "      <td>60.00</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>55164.6653</td>\n",
       "      <td>217472.733334</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2739.25</td>\n",
       "      <td>4822.0</td>\n",
       "      <td>8600.50</td>\n",
       "      <td>999000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>50353.5365</td>\n",
       "      <td>218560.171670</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>999000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>28.6417</td>\n",
       "      <td>6.726979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.00</td>\n",
       "      <td>28.0</td>\n",
       "      <td>33.00</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>304</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>8.8983</td>\n",
       "      <td>4.259809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>305</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>65.7844</td>\n",
       "      <td>20.581824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.00</td>\n",
       "      <td>70.0</td>\n",
       "      <td>79.00</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>48.2351</td>\n",
       "      <td>17.394622</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.00</td>\n",
       "      <td>49.0</td>\n",
       "      <td>59.00</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10.1628</td>\n",
       "      <td>13.923781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>5.4905</td>\n",
       "      <td>2.603226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>309</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>64.5862</td>\n",
       "      <td>10.993619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.00</td>\n",
       "      <td>67.0</td>\n",
       "      <td>72.00</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>310</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>8.7554</td>\n",
       "      <td>6.408859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>43.9539</td>\n",
       "      <td>10.918035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.00</td>\n",
       "      <td>46.0</td>\n",
       "      <td>51.00</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.1220</td>\n",
       "      <td>0.353169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>2.8279</td>\n",
       "      <td>4.735475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>0.197522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.6739</td>\n",
       "      <td>0.887602</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>5.7426</td>\n",
       "      <td>11.324106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.5708</td>\n",
       "      <td>1.925346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.1488</td>\n",
       "      <td>0.742370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>6.8697</td>\n",
       "      <td>6.279735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>12.2630</td>\n",
       "      <td>7.970495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>31.9589</td>\n",
       "      <td>7.150435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.00</td>\n",
       "      <td>32.0</td>\n",
       "      <td>36.00</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>322</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9.9545</td>\n",
       "      <td>11.649223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.00</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>74.3076</td>\n",
       "      <td>22.485172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.00</td>\n",
       "      <td>82.0</td>\n",
       "      <td>91.00</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>46.7196</td>\n",
       "      <td>14.360836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.00</td>\n",
       "      <td>51.0</td>\n",
       "      <td>57.00</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>13.1273</td>\n",
       "      <td>7.086390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>17.00</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>77.6779</td>\n",
       "      <td>21.731907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.00</td>\n",
       "      <td>85.0</td>\n",
       "      <td>93.00</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>14.2455</td>\n",
       "      <td>5.927136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>6.1302</td>\n",
       "      <td>2.448928</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>3.3054</td>\n",
       "      <td>1.857546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>330</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>22.8121</td>\n",
       "      <td>18.892858</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>17.0</td>\n",
       "      <td>36.00</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>20.5576</td>\n",
       "      <td>8.910287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>26.00</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>277.0525</td>\n",
       "      <td>124.722781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>197.00</td>\n",
       "      <td>252.0</td>\n",
       "      <td>323.00</td>\n",
       "      <td>1651.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>167.9049</td>\n",
       "      <td>87.244324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.00</td>\n",
       "      <td>150.0</td>\n",
       "      <td>194.00</td>\n",
       "      <td>1580.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>334 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature  missing    count         mean            std       min  \\\n",
       "0          0      0.0  10000.0  520579.2288  499079.648645       0.0   \n",
       "1          1      0.0  10000.0      46.6708       9.257170      18.0   \n",
       "2          2      0.0  10000.0  243557.5701  428964.747884       1.0   \n",
       "3          3      0.0  10000.0      46.8831      10.578271      18.0   \n",
       "4          4      0.0  10000.0  196027.3000     950.497267  190000.0   \n",
       "5          5      0.0  10000.0  239960.8596  426798.361930       1.0   \n",
       "6          6      0.0  10000.0  430870.8259  494785.452455       1.0   \n",
       "7          7      0.0  10000.0  706594.0515  454567.505278       1.0   \n",
       "8          8      0.0  10000.0  302101.4378  458858.669779       1.0   \n",
       "9          9      0.0  10000.0  542557.5240  497664.991029       1.0   \n",
       "10        10      0.0  10000.0      52.8758      21.389131       0.0   \n",
       "11        11      0.0  10000.0       2.1090       1.132805       1.0   \n",
       "12        12      0.0  10000.0  769635.8644  420164.563919      10.0   \n",
       "13        13      0.0  10000.0      47.9219      33.206353       0.0   \n",
       "14        14      0.0  10000.0  520579.1544  499079.726258       0.0   \n",
       "15        15      0.0  10000.0     182.9210     107.950188      10.0   \n",
       "16        16      0.0  10000.0     449.3776      39.947438       0.0   \n",
       "17        17      0.0  10000.0     482.3560      36.297088       0.0   \n",
       "18        18      0.0  10000.0     358.7217      40.975172       0.0   \n",
       "19        19      0.0  10000.0       5.8874       2.376149       0.0   \n",
       "20        20      0.0  10000.0     702.7789     248.323896       0.0   \n",
       "21        21      0.0  10000.0     313.2410      29.777753       0.0   \n",
       "22        22      0.0  10000.0     619.9018     213.735578       0.0   \n",
       "23        23      0.0  10000.0     262.1950      39.025687       0.0   \n",
       "24        24      0.0  10000.0    1810.8304    1134.881939       0.0   \n",
       "25        25      0.0  10000.0       4.4971       1.126818       0.0   \n",
       "26        26      0.0  10000.0      54.5342       8.657131       0.0   \n",
       "27        27      0.0  10000.0   55164.6653  217472.733334     100.0   \n",
       "28        28      0.0  10000.0   50353.5365  218560.171670       0.0   \n",
       "29        29      0.0  10000.0      28.6417       6.726979       0.0   \n",
       "..       ...      ...      ...          ...            ...       ...   \n",
       "304      304      0.0  10000.0       8.8983       4.259809       0.0   \n",
       "305      305      0.0  10000.0      65.7844      20.581824       0.0   \n",
       "306      306      0.0  10000.0      48.2351      17.394622       0.0   \n",
       "307      307      0.0  10000.0      10.1628      13.923781       0.0   \n",
       "308      308      0.0  10000.0       5.4905       2.603226       0.0   \n",
       "309      309      0.0  10000.0      64.5862      10.993619       0.0   \n",
       "310      310      0.0  10000.0       8.7554       6.408859       0.0   \n",
       "311      311      0.0  10000.0      43.9539      10.918035       0.0   \n",
       "312      312      0.0  10000.0       0.1220       0.353169       0.0   \n",
       "313      313      0.0  10000.0       2.8279       4.735475       0.0   \n",
       "314      314      0.0  10000.0       0.0359       0.197522       0.0   \n",
       "315      315      0.0  10000.0       0.6739       0.887602       0.0   \n",
       "316      316      0.0  10000.0       5.7426      11.324106       0.0   \n",
       "317      317      0.0  10000.0       0.5708       1.925346       0.0   \n",
       "318      318      0.0  10000.0       0.1488       0.742370       0.0   \n",
       "319      319      0.0  10000.0       6.8697       6.279735       0.0   \n",
       "320      320      0.0  10000.0      12.2630       7.970495       0.0   \n",
       "321      321      0.0  10000.0      31.9589       7.150435       0.0   \n",
       "322      322      0.0  10000.0       9.9545      11.649223       0.0   \n",
       "323      323      0.0  10000.0      74.3076      22.485172       0.0   \n",
       "324      324      0.0  10000.0      46.7196      14.360836       0.0   \n",
       "325      325      0.0  10000.0      13.1273       7.086390       0.0   \n",
       "326      326      0.0  10000.0      77.6779      21.731907       0.0   \n",
       "327      327      0.0  10000.0      14.2455       5.927136       0.0   \n",
       "328      328      0.0  10000.0       6.1302       2.448928       0.0   \n",
       "329      329      0.0  10000.0       3.3054       1.857546       0.0   \n",
       "330      330      0.0  10000.0      22.8121      18.892858       0.0   \n",
       "331      331      0.0  10000.0      20.5576       8.910287       0.0   \n",
       "332      332      0.0  10000.0     277.0525     124.722781       0.0   \n",
       "333      333      0.0  10000.0     167.9049      87.244324       0.0   \n",
       "\n",
       "           25%       50%        75%       max  \n",
       "0         1.00  999000.0  999000.00  999000.0  \n",
       "1        46.00      46.0      46.00      99.0  \n",
       "2         1.00       1.0       7.00  999000.0  \n",
       "3        42.00      46.0      52.00      99.0  \n",
       "4    196000.00  196000.0  196000.00  199000.0  \n",
       "5         1.00       2.0       2.00  999000.0  \n",
       "6         4.00       4.0  999000.00  999000.0  \n",
       "7         8.00  999000.0  999000.00  999000.0  \n",
       "8         5.00       7.0  999000.00  999000.0  \n",
       "9         1.00  999000.0  999000.00  999000.0  \n",
       "10       40.00      50.0      61.00     200.0  \n",
       "11        1.00       2.0       2.00       9.0  \n",
       "12   999000.00  999000.0  999000.00  999000.0  \n",
       "13       21.00      42.0      73.00     133.0  \n",
       "14        1.00  999000.0  999000.00  999000.0  \n",
       "15      170.00     170.0     170.00    2100.0  \n",
       "16      428.00     449.0     471.00     811.0  \n",
       "17      462.00     482.0     503.00     811.0  \n",
       "18      334.00     356.0     380.00     811.0  \n",
       "19        4.00       6.0       7.00      22.0  \n",
       "20      528.00     650.0     822.25    2162.0  \n",
       "21      300.00     310.0     330.00     660.0  \n",
       "22      467.00     580.0     725.00    2022.0  \n",
       "23      240.00     260.0     280.00     630.0  \n",
       "24     1114.00    1513.0    2136.00   11839.0  \n",
       "25        4.00       4.0       5.00      14.0  \n",
       "26       50.00      55.0      60.00      86.0  \n",
       "27     2739.25    4822.0    8600.50  999000.0  \n",
       "28        2.00       4.0       7.00  999000.0  \n",
       "29       24.00      28.0      33.00      56.0  \n",
       "..         ...       ...        ...       ...  \n",
       "304       6.00       8.0      11.00      32.0  \n",
       "305      57.00      70.0      79.00      99.0  \n",
       "306      38.00      49.0      59.00      99.0  \n",
       "307       2.00       5.0      11.00      94.0  \n",
       "308       4.00       5.0       7.00      23.0  \n",
       "309      59.00      67.0      72.00      99.0  \n",
       "310       4.00       7.0      11.00      45.0  \n",
       "311      38.00      46.0      51.00      99.0  \n",
       "312       0.00       0.0       0.00       4.0  \n",
       "313       0.00       1.0       3.00      42.0  \n",
       "314       0.00       0.0       0.00       3.0  \n",
       "315       0.00       1.0       1.00      61.0  \n",
       "316       1.00       2.0       5.00      73.0  \n",
       "317       0.00       0.0       0.00      29.0  \n",
       "318       0.00       0.0       0.00      19.0  \n",
       "319       4.00       5.0       8.00      89.0  \n",
       "320       7.00      11.0      15.00      99.0  \n",
       "321      28.00      32.0      36.00      99.0  \n",
       "322       3.00       6.0      12.00      99.0  \n",
       "323      65.00      82.0      91.00      99.0  \n",
       "324      41.00      51.0      57.00      80.0  \n",
       "325       9.00      12.0      17.00      91.0  \n",
       "326      70.00      85.0      93.00      99.0  \n",
       "327      11.00      15.0      18.00      49.0  \n",
       "328       4.00       6.0       8.00      40.0  \n",
       "329       2.00       3.0       4.00      29.0  \n",
       "330       7.00      17.0      36.00      91.0  \n",
       "331      14.00      20.0      26.00      76.0  \n",
       "332     197.00     252.0     323.00    1651.0  \n",
       "333     117.00     150.0     194.00    1580.0  \n",
       "\n",
       "[334 rows x 10 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get basic statistics for continuous features\n",
    "numeric = data_train.describe(include=['number']).T.reset_index()\n",
    "numeric.rename(columns={'index':'feature'},inplace=True)\n",
    "numeric.insert(1,'missing',(rows - numeric['count'])/ float(rows))\n",
    "numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>missing</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [feature, missing, count, mean, std, min, 25%, 50%, 75%, max]\n",
       "Index: []"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Examine features with count missing > 0\n",
    "numeric[numeric['missing']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>missing</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>523113.7876</td>\n",
       "      <td>498903.221672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>998880.0</td>\n",
       "      <td>998880.00</td>\n",
       "      <td>998880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>46.6103</td>\n",
       "      <td>9.306336</td>\n",
       "      <td>18.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.00</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>237535.0535</td>\n",
       "      <td>425279.685549</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>998880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>46.7921</td>\n",
       "      <td>10.692582</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>52.00</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>196012.9900</td>\n",
       "      <td>949.098856</td>\n",
       "      <td>190100.0</td>\n",
       "      <td>196000.0</td>\n",
       "      <td>196010.0</td>\n",
       "      <td>196110.00</td>\n",
       "      <td>199110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>228544.8079</td>\n",
       "      <td>419610.120297</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>998880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>420730.4115</td>\n",
       "      <td>493221.337666</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>998880.00</td>\n",
       "      <td>998880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>709405.8966</td>\n",
       "      <td>453181.921680</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>998880.0</td>\n",
       "      <td>998880.00</td>\n",
       "      <td>998880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>290278.4464</td>\n",
       "      <td>453550.895657</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>998880.00</td>\n",
       "      <td>998880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>540694.3683</td>\n",
       "      <td>497757.514610</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>998880.0</td>\n",
       "      <td>998880.00</td>\n",
       "      <td>998880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>53.3554</td>\n",
       "      <td>21.762198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>62.00</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>2.1311</td>\n",
       "      <td>1.178665</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>762151.9985</td>\n",
       "      <td>424775.484280</td>\n",
       "      <td>10.0</td>\n",
       "      <td>998880.0</td>\n",
       "      <td>998880.0</td>\n",
       "      <td>998880.00</td>\n",
       "      <td>998880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>48.1865</td>\n",
       "      <td>33.230467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>73.00</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>523113.7072</td>\n",
       "      <td>498903.305982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>998880.0</td>\n",
       "      <td>998880.00</td>\n",
       "      <td>998880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>184.8570</td>\n",
       "      <td>105.946411</td>\n",
       "      <td>10.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.00</td>\n",
       "      <td>2070.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>448.8379</td>\n",
       "      <td>41.917630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>427.0</td>\n",
       "      <td>449.0</td>\n",
       "      <td>470.00</td>\n",
       "      <td>811.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>481.4003</td>\n",
       "      <td>39.232886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>461.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>502.00</td>\n",
       "      <td>811.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>357.5846</td>\n",
       "      <td>42.647817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>379.00</td>\n",
       "      <td>811.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>5.9231</td>\n",
       "      <td>2.425443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>705.1332</td>\n",
       "      <td>250.392497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>650.0</td>\n",
       "      <td>826.00</td>\n",
       "      <td>2119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>313.6060</td>\n",
       "      <td>30.953070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>330.00</td>\n",
       "      <td>490.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>623.5115</td>\n",
       "      <td>216.272035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>731.25</td>\n",
       "      <td>1953.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>263.2120</td>\n",
       "      <td>39.061690</td>\n",
       "      <td>0.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>490.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>1809.0704</td>\n",
       "      <td>1114.809833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1114.0</td>\n",
       "      <td>1513.0</td>\n",
       "      <td>2152.00</td>\n",
       "      <td>11399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>4.4930</td>\n",
       "      <td>1.114664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>54.7187</td>\n",
       "      <td>8.807845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>60.00</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>57598.5878</td>\n",
       "      <td>222268.201619</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2760.0</td>\n",
       "      <td>4945.5</td>\n",
       "      <td>8712.00</td>\n",
       "      <td>998880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>52744.7710</td>\n",
       "      <td>223393.764159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>998880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>28.7643</td>\n",
       "      <td>6.799204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>33.00</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>304</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>8.8911</td>\n",
       "      <td>4.258269</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>305</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>66.3284</td>\n",
       "      <td>20.646336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>80.00</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>48.6492</td>\n",
       "      <td>17.598833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>60.00</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10.1982</td>\n",
       "      <td>14.136771</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>5.4773</td>\n",
       "      <td>2.627770</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>309</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>64.6269</td>\n",
       "      <td>11.127312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>72.00</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>310</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>8.8361</td>\n",
       "      <td>6.467056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>44.1383</td>\n",
       "      <td>10.862991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>51.00</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.1245</td>\n",
       "      <td>0.352437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>2.6929</td>\n",
       "      <td>4.608309</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0339</td>\n",
       "      <td>0.190143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.6829</td>\n",
       "      <td>0.683809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>5.5306</td>\n",
       "      <td>11.092609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.5862</td>\n",
       "      <td>1.994434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.1494</td>\n",
       "      <td>0.789393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>6.8654</td>\n",
       "      <td>6.530631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>12.2378</td>\n",
       "      <td>7.897942</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>32.1258</td>\n",
       "      <td>7.312737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>36.00</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>322</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9.7671</td>\n",
       "      <td>11.629092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.00</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>74.5170</td>\n",
       "      <td>22.422698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>91.00</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>46.8196</td>\n",
       "      <td>14.373848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>57.00</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>13.0381</td>\n",
       "      <td>7.147038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>17.00</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>77.8579</td>\n",
       "      <td>21.658846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>93.00</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>14.4377</td>\n",
       "      <td>5.962355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.00</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>6.0592</td>\n",
       "      <td>2.455748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>3.3431</td>\n",
       "      <td>1.881100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>330</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>23.6600</td>\n",
       "      <td>19.226642</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>37.00</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>20.3576</td>\n",
       "      <td>9.013548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>26.00</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>280.1397</td>\n",
       "      <td>129.188948</td>\n",
       "      <td>0.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>327.00</td>\n",
       "      <td>1651.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>170.2853</td>\n",
       "      <td>98.367942</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>198.00</td>\n",
       "      <td>3326.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>334 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature  missing    count         mean            std       min  \\\n",
       "0          0      0.0  10000.0  523113.7876  498903.221672       0.0   \n",
       "1          1      0.0  10000.0      46.6103       9.306336      18.0   \n",
       "2          2      0.0  10000.0  237535.0535  425279.685549       1.0   \n",
       "3          3      0.0  10000.0      46.7921      10.692582      18.0   \n",
       "4          4      0.0  10000.0  196012.9900     949.098856  190100.0   \n",
       "5          5      0.0  10000.0  228544.8079  419610.120297       1.0   \n",
       "6          6      0.0  10000.0  420730.4115  493221.337666       1.0   \n",
       "7          7      0.0  10000.0  709405.8966  453181.921680       1.0   \n",
       "8          8      0.0  10000.0  290278.4464  453550.895657       1.0   \n",
       "9          9      0.0  10000.0  540694.3683  497757.514610       1.0   \n",
       "10        10      0.0  10000.0      53.3554      21.762198       0.0   \n",
       "11        11      0.0  10000.0       2.1311       1.178665       1.0   \n",
       "12        12      0.0  10000.0  762151.9985  424775.484280      10.0   \n",
       "13        13      0.0  10000.0      48.1865      33.230467       0.0   \n",
       "14        14      0.0  10000.0  523113.7072  498903.305982       0.0   \n",
       "15        15      0.0  10000.0     184.8570     105.946411      10.0   \n",
       "16        16      0.0  10000.0     448.8379      41.917630       0.0   \n",
       "17        17      0.0  10000.0     481.4003      39.232886       0.0   \n",
       "18        18      0.0  10000.0     357.5846      42.647817       0.0   \n",
       "19        19      0.0  10000.0       5.9231       2.425443       0.0   \n",
       "20        20      0.0  10000.0     705.1332     250.392497       0.0   \n",
       "21        21      0.0  10000.0     313.6060      30.953070       0.0   \n",
       "22        22      0.0  10000.0     623.5115     216.272035       0.0   \n",
       "23        23      0.0  10000.0     263.2120      39.061690       0.0   \n",
       "24        24      0.0  10000.0    1809.0704    1114.809833       0.0   \n",
       "25        25      0.0  10000.0       4.4930       1.114664       0.0   \n",
       "26        26      0.0  10000.0      54.7187       8.807845       0.0   \n",
       "27        27      0.0  10000.0   57598.5878  222268.201619     100.0   \n",
       "28        28      0.0  10000.0   52744.7710  223393.764159       0.0   \n",
       "29        29      0.0  10000.0      28.7643       6.799204       0.0   \n",
       "..       ...      ...      ...          ...            ...       ...   \n",
       "304      304      0.0  10000.0       8.8911       4.258269       0.0   \n",
       "305      305      0.0  10000.0      66.3284      20.646336       0.0   \n",
       "306      306      0.0  10000.0      48.6492      17.598833       0.0   \n",
       "307      307      0.0  10000.0      10.1982      14.136771       0.0   \n",
       "308      308      0.0  10000.0       5.4773       2.627770       0.0   \n",
       "309      309      0.0  10000.0      64.6269      11.127312       0.0   \n",
       "310      310      0.0  10000.0       8.8361       6.467056       0.0   \n",
       "311      311      0.0  10000.0      44.1383      10.862991       0.0   \n",
       "312      312      0.0  10000.0       0.1245       0.352437       0.0   \n",
       "313      313      0.0  10000.0       2.6929       4.608309       0.0   \n",
       "314      314      0.0  10000.0       0.0339       0.190143       0.0   \n",
       "315      315      0.0  10000.0       0.6829       0.683809       0.0   \n",
       "316      316      0.0  10000.0       5.5306      11.092609       0.0   \n",
       "317      317      0.0  10000.0       0.5862       1.994434       0.0   \n",
       "318      318      0.0  10000.0       0.1494       0.789393       0.0   \n",
       "319      319      0.0  10000.0       6.8654       6.530631       0.0   \n",
       "320      320      0.0  10000.0      12.2378       7.897942       0.0   \n",
       "321      321      0.0  10000.0      32.1258       7.312737       0.0   \n",
       "322      322      0.0  10000.0       9.7671      11.629092       0.0   \n",
       "323      323      0.0  10000.0      74.5170      22.422698       0.0   \n",
       "324      324      0.0  10000.0      46.8196      14.373848       0.0   \n",
       "325      325      0.0  10000.0      13.0381       7.147038       0.0   \n",
       "326      326      0.0  10000.0      77.8579      21.658846       0.0   \n",
       "327      327      0.0  10000.0      14.4377       5.962355       0.0   \n",
       "328      328      0.0  10000.0       6.0592       2.455748       0.0   \n",
       "329      329      0.0  10000.0       3.3431       1.881100       0.0   \n",
       "330      330      0.0  10000.0      23.6600      19.226642       0.0   \n",
       "331      331      0.0  10000.0      20.3576       9.013548       0.0   \n",
       "332      332      0.0  10000.0     280.1397     129.188948       0.0   \n",
       "333      333      0.0  10000.0     170.2853      98.367942       0.0   \n",
       "\n",
       "          25%       50%        75%       max  \n",
       "0         1.0  998880.0  998880.00  998880.0  \n",
       "1        46.0      46.0      46.00      99.0  \n",
       "2         1.0       1.0       6.00  998880.0  \n",
       "3        42.0      46.0      52.00      99.0  \n",
       "4    196000.0  196010.0  196110.00  199110.0  \n",
       "5         1.0       2.0       2.00  998880.0  \n",
       "6         4.0       4.0  998880.00  998880.0  \n",
       "7         8.0  998880.0  998880.00  998880.0  \n",
       "8         5.0       7.0  998880.00  998880.0  \n",
       "9         1.0  998880.0  998880.00  998880.0  \n",
       "10       40.0      50.0      62.00     200.0  \n",
       "11        1.0       2.0       2.00       9.0  \n",
       "12   998880.0  998880.0  998880.00  998880.0  \n",
       "13       21.0      42.0      73.00     133.0  \n",
       "14        1.0  998880.0  998880.00  998880.0  \n",
       "15      170.0     170.0     170.00    2070.0  \n",
       "16      427.0     449.0     470.00     811.0  \n",
       "17      461.0     482.0     502.00     811.0  \n",
       "18      334.0     356.0     379.00     811.0  \n",
       "19        4.0       6.0       7.00      23.0  \n",
       "20      531.0     650.0     826.00    2119.0  \n",
       "21      300.0     310.0     330.00     490.0  \n",
       "22      472.0     580.0     731.25    1953.0  \n",
       "23      240.0     260.0     280.00     490.0  \n",
       "24     1114.0    1513.0    2152.00   11399.0  \n",
       "25        4.0       4.0       5.00      16.0  \n",
       "26       50.0      55.0      60.00      86.0  \n",
       "27     2760.0    4945.5    8712.00  998880.0  \n",
       "28        2.0       4.0       7.00  998880.0  \n",
       "29       24.0      28.0      33.00      65.0  \n",
       "..        ...       ...        ...       ...  \n",
       "304       6.0       8.0      11.00      33.0  \n",
       "305      58.0      70.0      80.00      99.0  \n",
       "306      38.0      49.0      60.00      99.0  \n",
       "307       2.0       5.0      11.00      93.0  \n",
       "308       4.0       5.0       7.00      22.0  \n",
       "309      59.0      67.0      72.00      99.0  \n",
       "310       4.0       7.0      11.00      53.0  \n",
       "311      38.0      46.0      51.00      99.0  \n",
       "312       0.0       0.0       0.00       6.0  \n",
       "313       0.0       1.0       3.00      59.0  \n",
       "314       0.0       0.0       0.00       3.0  \n",
       "315       0.0       1.0       1.00      24.0  \n",
       "316       0.0       2.0       5.00      73.0  \n",
       "317       0.0       0.0       0.00      28.0  \n",
       "318       0.0       0.0       0.00      27.0  \n",
       "319       4.0       5.0       8.00      97.0  \n",
       "320       7.0      11.0      15.00      94.0  \n",
       "321      28.0      32.0      36.00      99.0  \n",
       "322       3.0       6.0      12.00      99.0  \n",
       "323      65.0      82.0      91.00      99.0  \n",
       "324      41.0      51.0      57.00      80.0  \n",
       "325       9.0      12.0      17.00      91.0  \n",
       "326      71.0      85.0      93.00      99.0  \n",
       "327      11.0      15.0      19.00      34.0  \n",
       "328       4.0       6.0       7.00      40.0  \n",
       "329       2.0       3.0       4.00      20.0  \n",
       "330       8.0      17.0      37.00      99.0  \n",
       "331      14.0      20.0      26.00      94.0  \n",
       "332     198.0     252.0     327.00    1651.0  \n",
       "333     119.0     150.0     198.00    3326.0  \n",
       "\n",
       "[334 rows x 10 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get basic statistics for continuous features\n",
    "numeric1 = data_test.describe(include=['number']).T.reset_index()\n",
    "numeric1.rename(columns={'index':'feature'},inplace=True)\n",
    "numeric1.insert(1,'missing',(rows - numeric['count'])/ float(rows))\n",
    "numeric1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>missing</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [feature, missing, count, mean, std, min, 25%, 50%, 75%, max]\n",
       "Index: []"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Examine features with count missing > 0\n",
    "numeric1[numeric1['missing']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      False\n",
       "1      False\n",
       "2      False\n",
       "3      False\n",
       "4      False\n",
       "5      False\n",
       "6      False\n",
       "7      False\n",
       "8      False\n",
       "9      False\n",
       "10     False\n",
       "11     False\n",
       "12     False\n",
       "13     False\n",
       "14     False\n",
       "15     False\n",
       "16     False\n",
       "17     False\n",
       "18     False\n",
       "19     False\n",
       "20     False\n",
       "21     False\n",
       "22     False\n",
       "23     False\n",
       "24     False\n",
       "25     False\n",
       "26     False\n",
       "27     False\n",
       "28     False\n",
       "29     False\n",
       "       ...  \n",
       "304    False\n",
       "305    False\n",
       "306    False\n",
       "307    False\n",
       "308    False\n",
       "309    False\n",
       "310    False\n",
       "311    False\n",
       "312    False\n",
       "313    False\n",
       "314    False\n",
       "315    False\n",
       "316    False\n",
       "317    False\n",
       "318    False\n",
       "319    False\n",
       "320    False\n",
       "321    False\n",
       "322    False\n",
       "323    False\n",
       "324    False\n",
       "325    False\n",
       "326    False\n",
       "327    False\n",
       "328    False\n",
       "329    False\n",
       "330    False\n",
       "331    False\n",
       "332    False\n",
       "333    False\n",
       "Length: 334, dtype: bool"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      False\n",
       "1      False\n",
       "2      False\n",
       "3      False\n",
       "4      False\n",
       "5      False\n",
       "6      False\n",
       "7      False\n",
       "8      False\n",
       "9      False\n",
       "10     False\n",
       "11     False\n",
       "12     False\n",
       "13     False\n",
       "14     False\n",
       "15     False\n",
       "16     False\n",
       "17     False\n",
       "18     False\n",
       "19     False\n",
       "20     False\n",
       "21     False\n",
       "22     False\n",
       "23     False\n",
       "24     False\n",
       "25     False\n",
       "26     False\n",
       "27     False\n",
       "28     False\n",
       "29     False\n",
       "       ...  \n",
       "304    False\n",
       "305    False\n",
       "306    False\n",
       "307    False\n",
       "308    False\n",
       "309    False\n",
       "310    False\n",
       "311    False\n",
       "312    False\n",
       "313    False\n",
       "314    False\n",
       "315    False\n",
       "316    False\n",
       "317    False\n",
       "318    False\n",
       "319    False\n",
       "320    False\n",
       "321    False\n",
       "322    False\n",
       "323    False\n",
       "324    False\n",
       "325    False\n",
       "326    False\n",
       "327    False\n",
       "328    False\n",
       "329    False\n",
       "330    False\n",
       "331    False\n",
       "332    False\n",
       "333    False\n",
       "Length: 334, dtype: bool"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our data is very imbalanced. The majority class is 0 in both the training and test sets. This is going to cause a large amount of overfitting of the majority class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12ca4898>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD3CAYAAAAHQMOGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a0d128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_train_labels.iloc[:,0].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x13a022b0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD3CAYAAAAHQMOGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13c38128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_test_labels.iloc[:,0].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "First we will normalize the data in order to scale our data, it appears that there is a wide range of scales here with some attributes having ranges as small as 0 to 1 and others from 0 to 999000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler_train = StandardScaler()\n",
    "scaled_data_train = scaler_train.fit_transform(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.95865399, -0.07246637, -0.56780365, ...,  0.72306558,\n",
       "        -0.18483915, -0.66374284],\n",
       "       [-1.04312861,  0.35965273, -0.56780598, ...,  1.05977189,\n",
       "        -0.86638465, -0.27401319],\n",
       "       [ 0.95865399,  0.35965273, -0.56780598, ...,  0.38635927,\n",
       "         0.48066999, -0.72105603],\n",
       "       ..., \n",
       "       [-1.04312861,  0.79177183, -0.56780598, ...,  1.28424277,\n",
       "         0.43256089, -0.6178923 ],\n",
       "       [ 0.95865399, -0.28852593, -0.56780598, ..., -0.96046598,\n",
       "         0.40850635, -0.28547583],\n",
       "       [ 0.95865399, -0.07246637,  1.76117084, ..., -0.28705335,\n",
       "        -0.4334028 ,  0.17302965]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler_test = StandardScaler()\n",
    "scaled_data_test = scaler_test.fit_transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.04857801, -1.1401727 , -0.55855932, ..., -0.7053735 ,\n",
       "         0.51756473, -0.60271938],\n",
       "       [-1.04858001, -1.1401727 , -0.55856402, ..., -1.70392037,\n",
       "        -0.11719622,  0.36309071],\n",
       "       [ 0.95367194, -0.06558225, -0.55856402, ..., -0.81632316,\n",
       "         1.06717483, -0.27739387],\n",
       "       ..., \n",
       "       [ 0.95367194, -0.06558225,  1.79031128, ...,  0.40412301,\n",
       "         0.44015486,  1.237403  ],\n",
       "       [-1.04857801, -0.92525461, -0.55855226, ..., -1.37107141,\n",
       "        -0.05526833,  0.18009512],\n",
       "       [-1.04857801, -1.1401727 , -0.55856402, ..., -0.37252455,\n",
       "        -0.02430438, -0.37905809]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=200, score_func=<function f_classif at 0x000000000E3D3898>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "sel = SelectKBest(f_classif, k=200).fit(data_train, data_train_labels)\n",
    "#x_new = sel.transform(scaled_data_train)\n",
    "#scores = sel.scores_\n",
    "sel.fit(scaled_data_train, data_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000L, 200L)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel.transform(scaled_data_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.56539029e+00,   9.98619907e+01,   5.00670732e+00,\n",
       "         9.03267692e+01,   7.76235142e+01,   1.81950225e+01,\n",
       "         2.09170069e+01,   2.47030636e+01,   7.80754400e+00,\n",
       "         2.87491086e+01,   1.80922557e+01,   6.64476760e+00,\n",
       "         2.02025004e+01,   8.37813900e+00,   4.56538868e+00,\n",
       "         1.74163749e+00,   1.89140080e+01,   3.27897422e+01,\n",
       "         5.51664985e+00,   8.32382717e+01,   4.53518512e+01,\n",
       "         1.02396234e-01,   4.38504875e+01,   1.87168884e+00,\n",
       "         3.84809432e+01,   3.04116037e-01,   2.49940827e+00,\n",
       "         1.20994130e+01,   1.20434754e+01,   1.67284891e+01,\n",
       "         6.87801815e+00,   3.68592832e+01,   5.74020451e+01,\n",
       "         3.25073201e+01,   5.16058048e+01,   5.34265399e+00,\n",
       "         7.01254973e-01,   9.83835057e-03,   7.99275162e-02,\n",
       "         4.92857986e+00,   2.26313205e+00,   1.55132378e+01,\n",
       "         2.75437736e+01,   3.73635786e+00,   7.44758409e+01,\n",
       "         3.93966228e+01,   2.85370976e+01,   2.87251781e+01,\n",
       "         3.25898597e+01,   3.77540177e+01,   4.10746724e+01,\n",
       "         2.78708487e+01,   5.30673702e+00,   6.93204348e+00,\n",
       "         2.19402032e+00,   1.52051768e+00,   2.18845577e+00,\n",
       "         2.51968220e+01,   1.39165432e-02,   1.08049666e+01,\n",
       "         2.06596362e+01,   1.23313673e+01,   2.27253261e+00,\n",
       "         3.01658116e+01,   3.02914294e+00,   2.24383074e+01,\n",
       "         1.56944846e+01,   3.76884988e+01,   4.48221165e-01,\n",
       "         4.93975557e-01,   4.04922199e+01,   1.95618874e+00,\n",
       "         2.69475407e+00,   1.86643648e+00,   3.40602487e+01,\n",
       "         1.81699782e+01,   6.46195856e+00,   7.17950415e-01,\n",
       "         7.86144919e+00,   5.98839318e+01,   7.37102401e+01,\n",
       "         3.79738935e+01,   7.35417208e+00,   3.38800771e+01,\n",
       "         2.74961055e+00,   2.31214802e+00,   5.41100744e-02,\n",
       "         3.45548093e-02,   8.80573916e-01,   1.41602898e+01,\n",
       "         4.64374123e+00,   4.57728341e+00,   6.03666646e-02,\n",
       "         1.96817317e+00,   7.84004652e+00,   5.45201430e-02,\n",
       "         6.56196856e-01,   2.37427721e+01,   9.81935511e+00,\n",
       "         4.75936039e+00,   3.66157260e+00,   1.74074964e+01,\n",
       "         1.55677884e+01,   2.65162508e+00,   1.35887676e+01,\n",
       "         1.61118925e+00,   1.85606787e+00,   5.06370433e-01,\n",
       "         6.24014072e-01,   2.09713682e+01,   7.45220498e+00,\n",
       "         8.00840940e+00,   1.09766981e+01,   3.22222194e+00,\n",
       "         3.12066817e+01,   2.94966061e+01,   2.94813311e+01,\n",
       "         8.84237447e+00,   2.07358799e+01,   1.64226873e-01,\n",
       "         5.48196823e+00,   1.21766169e+00,   5.79968724e-01,\n",
       "         1.12296054e-01,   4.47605712e+01,   5.56130108e+00,\n",
       "         1.40750809e+01,   4.42108901e-01,   6.02504954e+01,\n",
       "         8.75580116e-01,   7.28453045e+01,   5.40411461e+01,\n",
       "         4.70064658e+00,   8.94777768e+00,   5.94064199e-02,\n",
       "         1.86288837e-01,   1.70245251e+00,   6.54493360e+00,\n",
       "         1.42291917e+01,   7.86780000e+00,   7.65207850e+00,\n",
       "         9.04420063e+00,   2.20157581e+01,   3.41642527e+00,\n",
       "         3.81768715e+01,   3.55639195e+01,   3.39832005e+01,\n",
       "         5.30489438e+01,   5.21343152e+01,   3.33972172e+01,\n",
       "         8.30715715e-01,   3.44873230e+01,   1.75553879e+01,\n",
       "         9.16453543e-01,   2.61494085e+01,   1.34405416e-01,\n",
       "         1.79880980e+00,   3.00757127e+01,   9.77708734e-01,\n",
       "         3.05652339e+01,   4.74907046e-01,   1.17766956e-02,\n",
       "         7.75899420e+00,   2.62319305e+01,   1.80349203e+01,\n",
       "         1.63657233e-01,   6.18358164e-01,   7.31541089e-01,\n",
       "         9.01892149e+00,   2.65758651e+01,   2.68865603e-01,\n",
       "         1.79884481e+01,   6.37091658e+00,   7.04359920e-02,\n",
       "         1.41690607e+01,   3.85892415e+01,   5.80474765e+00,\n",
       "         1.82065053e-01,   3.73532142e+00,   2.87510780e+01,\n",
       "         2.34171887e+01,   9.07844227e+00,   4.18815374e+01,\n",
       "         3.26585589e+01,   2.91270759e+01,   6.38129084e+01,\n",
       "         3.63904795e+01,   9.79182000e+00,   7.44208857e+00,\n",
       "         3.91811941e+01,   3.42761070e+01,   8.18126589e+00,\n",
       "         3.68152958e+00,   1.98852371e-01,   2.04896337e+01,\n",
       "         5.27020708e+01,   2.28558916e+00,   2.18042640e+00,\n",
       "         9.34253916e+00,   1.74718287e+00,   1.07491082e+01,\n",
       "         1.37127664e+00,   1.56246410e+00,   1.22616315e+01,\n",
       "         7.62768734e+01,   4.86710210e+01,   3.71455421e+01,\n",
       "         1.74725894e+01,   8.11803297e+01,   9.29064873e+01,\n",
       "         3.52673039e+01,   4.00811577e+00,   2.67953862e+00,\n",
       "         2.07286529e+00,   2.70843295e+01,   9.59135230e-05,\n",
       "         2.84311348e-01,   1.25559409e+01,   2.25147323e+01,\n",
       "         1.95799765e-02,   1.87105341e+00,   3.04054648e+01,\n",
       "         1.12109990e+01,   3.23665195e+01,   1.94855460e+01,\n",
       "         5.24057694e+01,   4.79971749e+00,   3.91842941e+00,\n",
       "         1.77941783e-02,   6.05914703e+01,   1.39576437e+00,\n",
       "         3.57097894e+01,   1.11727220e+00,   4.66049129e+01,\n",
       "         1.52666490e+01,   1.88094809e-01,   9.65020184e+00,\n",
       "         4.78889502e+01,   1.97777581e+00,   3.02386568e+00,\n",
       "         3.27243861e+00,   3.68422288e+01,   1.11803808e-01,\n",
       "         1.61530253e+01,   5.46016678e-01,   3.56893515e+01,\n",
       "         9.50660277e+00,   7.23827996e-02,   6.38006232e+00,\n",
       "         1.67465957e+01,   2.92046923e+01,   3.42739259e+01,\n",
       "         3.36038545e+01,   1.74446983e+01,   2.28840450e+01,\n",
       "         2.84908613e+01,   2.69008331e+01,   2.21748242e+01,\n",
       "         3.27214120e-04,   8.71897246e+00,   1.35358383e+01,\n",
       "         2.02875060e+01,   5.30379508e+00,   1.22036066e+01,\n",
       "         1.85217000e+01,   3.02560563e+00,   1.06612661e+01,\n",
       "         3.23674405e-01,   7.51548760e+00,   7.13143080e+00,\n",
       "         9.40105711e+00,   4.52819097e+00,   4.21052687e+00,\n",
       "         2.61143938e+00,   1.27697564e+01,   1.46567347e+01,\n",
       "         1.04394433e+01,   1.04376370e+01,   1.21501617e-01,\n",
       "         7.27502799e+01,   7.78380173e+01,   2.45967450e+01,\n",
       "         1.27645551e+00,   3.87522265e+00,   9.11908262e+01,\n",
       "         3.87123092e+01,   9.88970861e-01,   4.49188310e+00,\n",
       "         1.00458723e+00,   1.11280511e-03,   3.13542273e+01,\n",
       "         2.67741378e+01,   1.34998221e+01,   6.90541590e+01,\n",
       "         1.04794382e+02,   1.19199285e+02,   1.23456737e+01,\n",
       "         4.11704502e+01,   4.80707826e+01,   2.96805386e-01,\n",
       "         9.49795598e+00,   3.13813821e+00,   8.99170568e+00,\n",
       "         2.21326934e-01,   2.54777993e+00,   1.42883130e+01,\n",
       "         1.14172462e+00,   1.36706844e+00,   8.60209369e+01,\n",
       "         2.44190069e+01,   3.60300156e+00,   3.52205724e-02,\n",
       "         2.44109464e+01,   5.79282106e+00,   1.24306420e-02,\n",
       "         1.07255019e+00,   1.17660474e+01,   3.17155221e-02,\n",
       "         3.73889978e+00,   1.20169760e+01,   6.12837531e+00,\n",
       "         1.64438801e-02,   5.63380969e-01,   6.22586962e+00,\n",
       "         4.62907774e-01,   1.87063348e+01,   4.37039907e+00,\n",
       "         1.03856676e+01,   2.96287618e+01,   3.91264569e+00,\n",
       "         8.65262881e-01,   2.57686912e+00,   3.51507006e+01,\n",
       "         1.57329703e+01])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[295, 294]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import heapq\n",
    "heapq.nlargest(2, range(len(sel.scores_)), sel.scores_.take)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification using raw and balanced data\n",
    "\n",
    "Here two classification methods are used - Random Forest and Decision Trees, a comparison of the raw and scaled data unbalanced and balanced using the SMOTE method to over-sample the data are used. The accuracy scores, precision and recall results as well as f1 scores are examined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "print('scipy: %s' % scipy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "print('numpy: %s' % numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from imblearn.datasets import fetch_datasets\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "raw data distribution: Counter({0: 9091, 1: 909})\n",
      "SMOTE data distribution: Counter({0: 9091, 1: 9091})\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.98      0.94      9061\n",
      "          1       0.13      0.02      0.04       939\n",
      "\n",
      "avg / total       0.83      0.89      0.86     10000\n",
      "\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.91      0.95      0.08      0.93      0.28      0.08      9061\n",
      "          1       0.15      0.08      0.95      0.10      0.28      0.07       939\n",
      "\n",
      "avg / total       0.84      0.87      0.16      0.85      0.28      0.08     10000\n",
      "\n",
      "\n",
      "\n",
      "Raw Pipeline Score 0.9053\n",
      "SMOTE Pipeline Score 0.9053\n",
      "\n",
      "\n",
      "raw classification\n",
      "accuracy: 0.894\n",
      "precision: 0.128834355828\n",
      "recall: 0.0223642172524\n",
      "f1: 0.038112522686\n",
      "\n",
      "\n",
      "SMOTE classification\n",
      "accuracy: 0.8689\n",
      "precision: 0.145038167939\n",
      "recall: 0.0809371671991\n",
      "f1: 0.103896103896\n",
      "\n",
      "\n",
      "Scaled Data Classification\n",
      "accuracy: 0.8926\n",
      "precision: 0.135135135135\n",
      "recall: 0.0266240681576\n",
      "f1: 0.0444839857651\n",
      "\n",
      "\n",
      "SMOTE Scaled Classification\n",
      "accuracy: 0.4174\n",
      "precision: 0.102618311921\n",
      "recall: 0.671991480298\n",
      "f1: 0.178047404063\n"
     ]
    }
   ],
   "source": [
    "def print_results(headline, true_value, pred):\n",
    "    print(headline)\n",
    "    print('accuracy: {}'.format(accuracy_score(true_value, pred)))\n",
    "    print('precision: {}'.format(precision_score(true_value, pred)))\n",
    "    print('recall: {}'.format(recall_score(true_value, pred)))\n",
    "    print('f1: {}'.format(f1_score(true_value, pred)))\n",
    "    \n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "#raw data\n",
    "pipeline = make_pipeline(classifier)\n",
    "model = pipeline.fit(data_train, data_train_labels)\n",
    "prediction = model.predict(data_test)\n",
    "\n",
    "#raw balanced data using SMOTE\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), classifier)\n",
    "smote_model = smote_pipeline.fit(data_train, data_train_labels)\n",
    "smote_prediction = smote_model.predict(data_test)\n",
    "\n",
    "#Scaled data\n",
    "pipeline_scaledr = make_pipeline(classifier)\n",
    "model_scaledr = pipeline_scaledr.fit(scaled_data_train, data_train_labels)\n",
    "prediction_scaledr = model_scaledr.predict(scaled_data_test)\n",
    "\n",
    "#Scaled data using SMOTE\n",
    "smote_pipeline_scaledr = make_pipeline_imb(SMOTE(), classifier)\n",
    "smote_model_scaledr = smote_pipeline_scaledr.fit(scaled_data_train, data_train_labels)\n",
    "smote_prediction_scaledr = smote_model_scaledr.predict(scaled_data_test)\n",
    "\n",
    "print'\\n'\n",
    "print('raw data distribution: {}'.format(Counter(data_train_labels[0])))\n",
    "X_smote, y_smote = SMOTE().fit_sample(data_train, data_train_labels[0])\n",
    "print('SMOTE data distribution: {}'.format(Counter(y_smote)))\n",
    "print'\\n'\n",
    "\n",
    "\n",
    "print(classification_report(data_test_labels, prediction))\n",
    "print(classification_report_imbalanced(data_test_labels, smote_prediction))\n",
    "\n",
    "print'\\n'\n",
    "print('Raw Pipeline Score {}'.format(pipeline.score(data_test, data_test_labels)))\n",
    "print('SMOTE Pipeline Score {}'.format(smote_pipeline.score(data_test, data_test_labels)))\n",
    "\n",
    "print'\\n'\n",
    "print_results('raw classification', data_test_labels, prediction)\n",
    "print'\\n'\n",
    "print_results('SMOTE classification', data_test_labels, smote_prediction)\n",
    "print'\\n'\n",
    "print_results('Scaled Data Classification', data_test_labels, prediction_scaledr)\n",
    "print'\\n'\n",
    "print_results('SMOTE Scaled Classification', data_test_labels, smote_prediction_scaledr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Raw Data Distribution: Counter({0: 9091, 1: 909})\n",
      "SMOTE Data Distribution: Counter({0: 9091, 1: 9091})\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.90      0.90      9061\n",
      "          1       0.14      0.16      0.15       939\n",
      "\n",
      "avg / total       0.84      0.83      0.83     10000\n",
      "\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.91      0.87      0.17      0.89      0.38      0.16      9061\n",
      "          1       0.12      0.17      0.87      0.14      0.38      0.14       939\n",
      "\n",
      "avg / total       0.84      0.81      0.23      0.82      0.38      0.15     10000\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.87      0.89      9061\n",
      "          1       0.14      0.20      0.16       939\n",
      "\n",
      "avg / total       0.84      0.81      0.82     10000\n",
      "\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.90      0.07      0.92      0.13      0.26      0.06      9061\n",
      "          1       0.09      0.92      0.07      0.17      0.26      0.07       939\n",
      "\n",
      "avg / total       0.82      0.15      0.84      0.14      0.26      0.06     10000\n",
      "\n",
      "\n",
      "\n",
      "Raw Pipeline Score 0.3066\n",
      "SMOTE Pipeline Score 0.3066\n",
      "Scaled Pipeline Score 0.1513\n",
      "\n",
      "\n",
      "Raw Classification\n",
      "accuracy: 0.8272\n",
      "precision: 0.138405132906\n",
      "recall: 0.160809371672\n",
      "f1: 0.148768472906\n",
      "\n",
      "\n",
      "SMOTE Classification on Raw Data\n",
      "accuracy: 0.8083\n",
      "precision: 0.121517027864\n",
      "recall: 0.16719914803\n",
      "f1: 0.140744060959\n",
      "\n",
      "\n",
      "Scaled Data Classification\n",
      "accuracy: 0.8052\n",
      "precision: 0.137311286844\n",
      "recall: 0.203407880724\n",
      "f1: 0.163948497854\n",
      "\n",
      "\n",
      "SMOTE Scaled Classification\n",
      "accuracy: 0.1513\n",
      "precision: 0.0933189655172\n",
      "recall: 0.92225772098\n",
      "f1: 0.16948820824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def print_results(headline, true_value, pred):\n",
    "    print(headline)\n",
    "    print('accuracy: {}'.format(accuracy_score(true_value, pred)))\n",
    "    print('precision: {}'.format(precision_score(true_value, pred)))\n",
    "    print('recall: {}'.format(recall_score(true_value, pred)))\n",
    "    print('f1: {}'.format(f1_score(true_value, pred)))\n",
    "    \n",
    "classifier_tree = DecisionTreeClassifier()\n",
    "\n",
    "#raw data\n",
    "pipeline = make_pipeline(classifier_tree)\n",
    "model = pipeline.fit(data_train, data_train_labels)\n",
    "prediction = model.predict(data_test)\n",
    "\n",
    "#raw balanced data using SMOTE\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), classifier_tree)\n",
    "smote_model = smote_pipeline.fit(data_train, data_train_labels)\n",
    "smote_prediction = smote_model.predict(data_test)\n",
    "\n",
    "#Scaled data\n",
    "pipeline_scaled = make_pipeline(classifier_tree)\n",
    "model_scaled = pipeline_scaled.fit(scaled_data_train, data_train_labels)\n",
    "prediction_scaled = model_scaled.predict(scaled_data_test)\n",
    "\n",
    "#Scaled data using SMOTE\n",
    "smote_pipeline_scaled = make_pipeline_imb(SMOTE(), classifier_tree)\n",
    "smote_model_scaled = smote_pipeline_scaled.fit(scaled_data_train, data_train_labels)\n",
    "smote_prediction_scaled = smote_model_scaled.predict(scaled_data_test)\n",
    "\n",
    "\n",
    "print'\\n'\n",
    "print('Raw Data Distribution: {}'.format(Counter(data_train_labels[0])))\n",
    "X_smote, y_smote = SMOTE().fit_sample(data_train, data_train_labels[0])\n",
    "print('SMOTE Data Distribution: {}'.format(Counter(y_smote)))\n",
    "print'\\n'\n",
    "\n",
    "\n",
    "print(classification_report(data_test_labels, prediction))\n",
    "print(classification_report_imbalanced(data_test_labels, smote_prediction))\n",
    "print(classification_report(data_test_labels, prediction_scaled))\n",
    "print(classification_report_imbalanced(data_test_labels, smote_prediction_scaled))\n",
    "\n",
    "print'\\n'\n",
    "print('Raw Pipeline Score {}'.format(pipeline.score(data_test, data_test_labels)))\n",
    "print('SMOTE Pipeline Score {}'.format(smote_pipeline.score(data_test, data_test_labels)))\n",
    "print('Scaled Pipeline Score {}'.format(smote_pipeline_scaled.score(scaled_data_test, data_test_labels)))\n",
    "\n",
    "print'\\n'\n",
    "print_results('Raw Classification', data_test_labels, prediction)\n",
    "print'\\n'\n",
    "print_results('SMOTE Classification on Raw Data', data_test_labels, smote_prediction)\n",
    "print'\\n'\n",
    "print_results('Scaled Data Classification', data_test_labels, prediction_scaled)\n",
    "print'\\n'\n",
    "print_results('SMOTE Scaled Classification', data_test_labels, smote_prediction_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that in both classifiers, using SMOTE on the normalized data has a drastic effect on the recall. It also gets a better recall score on the raw data as well. The counter function above illustrates how SMOTE evens the data with over-sampling - the classes are equalized at 9,091 instances and class 1 can now be analyzed alongside class 0 much more evenly. The overall accuracy and precision suffer, but the recall is much improved. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
